{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Marketing team\n",
    "\n",
    "#  2.Data science process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# organizationâ€™s business climate and goals :\n",
    "\n",
    "The major features we found effect on customer to buy or not buy is the current_price and country and regular_price features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import cov\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set() # setting seaborn default for plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('full_gen_data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>article</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>retailweek</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>sizes</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Germany</td>\n",
       "      <td>UD3728</td>\n",
       "      <td>113</td>\n",
       "      <td>43.95</td>\n",
       "      <td>13.95</td>\n",
       "      <td>0.317406</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1824.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Germany</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>22</td>\n",
       "      <td>51.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.307026</td>\n",
       "      <td>2015-11-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xs,s,m,l,xl</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Austria</td>\n",
       "      <td>DI9187</td>\n",
       "      <td>16</td>\n",
       "      <td>20.95</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.331742</td>\n",
       "      <td>2016-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country article  sales  regular_price  current_price     ratio  \\\n",
       "0    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "1    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "2    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "3    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "4    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "5    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "6    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "7    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "8    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "9    Germany  YN8639     28           5.95           3.95  0.663866   \n",
       "10   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "11   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "12   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "13   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "14   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "15   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "16   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "17   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "18   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "19   Germany  CF3238     27          37.95          28.95  0.762846   \n",
       "20   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "21   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "22   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "23   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "24   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "25   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "26   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "27   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "28   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "29   Germany  WR9459     59          57.95          44.95  0.775669   \n",
       "..       ...     ...    ...            ...            ...       ...   \n",
       "470  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "471  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "472  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "473  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "474  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "475  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "476  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "477  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "478  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "479  Germany  UD3728    113          43.95          13.95  0.317406   \n",
       "480  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "481  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "482  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "483  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "484  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "485  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "486  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "487  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "488  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "489  Germany  VT7698     22          51.95          15.95  0.307026   \n",
       "490  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "491  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "492  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "493  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "494  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "495  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "496  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "497  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "498  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "499  Austria  DI9187     16          20.95           6.95  0.331742   \n",
       "\n",
       "     retailweek  promo1  promo2  customer_id  ...      style  \\\n",
       "0    2016-03-27       0       0       1003.0  ...       slim   \n",
       "1    2016-03-27       0       0       1003.0  ...    regular   \n",
       "2    2016-03-27       0       0       1003.0  ...    regular   \n",
       "3    2016-03-27       0       0       1003.0  ...    regular   \n",
       "4    2016-03-27       0       0       1003.0  ...    regular   \n",
       "5    2016-03-27       0       0       1003.0  ...       wide   \n",
       "6    2016-03-27       0       0       1003.0  ...       wide   \n",
       "7    2016-03-27       0       0       1003.0  ...       wide   \n",
       "8    2016-03-27       0       0       1003.0  ...       slim   \n",
       "9    2016-03-27       0       0       1003.0  ...    regular   \n",
       "10   2015-01-25       0       0       1649.0  ...       slim   \n",
       "11   2015-01-25       0       0       1649.0  ...    regular   \n",
       "12   2015-01-25       0       0       1649.0  ...    regular   \n",
       "13   2015-01-25       0       0       1649.0  ...    regular   \n",
       "14   2015-01-25       0       0       1649.0  ...    regular   \n",
       "15   2015-01-25       0       0       1649.0  ...       wide   \n",
       "16   2015-01-25       0       0       1649.0  ...       wide   \n",
       "17   2015-01-25       0       0       1649.0  ...       wide   \n",
       "18   2015-01-25       0       0       1649.0  ...       slim   \n",
       "19   2015-01-25       0       0       1649.0  ...    regular   \n",
       "20   2017-01-29       0       0        936.0  ...       slim   \n",
       "21   2017-01-29       0       0        936.0  ...    regular   \n",
       "22   2017-01-29       0       0        936.0  ...    regular   \n",
       "23   2017-01-29       0       0        936.0  ...    regular   \n",
       "24   2017-01-29       0       0        936.0  ...    regular   \n",
       "25   2017-01-29       0       0        936.0  ...       wide   \n",
       "26   2017-01-29       0       0        936.0  ...       wide   \n",
       "27   2017-01-29       0       0        936.0  ...       wide   \n",
       "28   2017-01-29       0       0        936.0  ...       slim   \n",
       "29   2017-01-29       0       0        936.0  ...    regular   \n",
       "..          ...     ...     ...          ...  ...        ...   \n",
       "470  2017-04-09       0       0       1824.0  ...       slim   \n",
       "471  2017-04-09       0       0       1824.0  ...    regular   \n",
       "472  2017-04-09       0       0       1824.0  ...    regular   \n",
       "473  2017-04-09       0       0       1824.0  ...    regular   \n",
       "474  2017-04-09       0       0       1824.0  ...    regular   \n",
       "475  2017-04-09       0       0       1824.0  ...       wide   \n",
       "476  2017-04-09       0       0       1824.0  ...       wide   \n",
       "477  2017-04-09       0       0       1824.0  ...       wide   \n",
       "478  2017-04-09       0       0       1824.0  ...       slim   \n",
       "479  2017-04-09       0       0       1824.0  ...    regular   \n",
       "480  2015-11-08       0       0       1752.0  ...       slim   \n",
       "481  2015-11-08       0       0       1752.0  ...    regular   \n",
       "482  2015-11-08       0       0       1752.0  ...    regular   \n",
       "483  2015-11-08       0       0       1752.0  ...    regular   \n",
       "484  2015-11-08       0       0       1752.0  ...    regular   \n",
       "485  2015-11-08       0       0       1752.0  ...       wide   \n",
       "486  2015-11-08       0       0       1752.0  ...       wide   \n",
       "487  2015-11-08       0       0       1752.0  ...       wide   \n",
       "488  2015-11-08       0       0       1752.0  ...       slim   \n",
       "489  2015-11-08       0       0       1752.0  ...    regular   \n",
       "490  2016-10-09       0       0       4739.0  ...       slim   \n",
       "491  2016-10-09       0       0       4739.0  ...    regular   \n",
       "492  2016-10-09       0       0       4739.0  ...    regular   \n",
       "493  2016-10-09       0       0       4739.0  ...    regular   \n",
       "494  2016-10-09       0       0       4739.0  ...    regular   \n",
       "495  2016-10-09       0       0       4739.0  ...       wide   \n",
       "496  2016-10-09       0       0       4739.0  ...       wide   \n",
       "497  2016-10-09       0       0       4739.0  ...       wide   \n",
       "498  2016-10-09       0       0       4739.0  ...       slim   \n",
       "499  2016-10-09       0       0       4739.0  ...    regular   \n",
       "\n",
       "                   sizes  gender  rgb_r_main_col rgb_g_main_col  \\\n",
       "0    xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "1    xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "2    xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "3    xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "4    xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "5    xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "6            xs,s,m,l,xl  unisex             139             26   \n",
       "7    xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "8    xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "9    xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "10   xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "11   xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "12   xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "13   xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "14   xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "15   xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "16           xs,s,m,l,xl  unisex             139             26   \n",
       "17   xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "18   xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "19   xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "20   xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "21   xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "22   xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "23   xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "24   xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "25   xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "26           xs,s,m,l,xl  unisex             139             26   \n",
       "27   xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "28   xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "29   xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "..                   ...     ...             ...            ...   \n",
       "470  xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "471  xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "472  xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "473  xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "474  xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "475  xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "476          xs,s,m,l,xl  unisex             139             26   \n",
       "477  xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "478  xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "479  xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "480  xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "481  xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "482  xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "483  xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "484  xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "485  xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "486          xs,s,m,l,xl  unisex             139             26   \n",
       "487  xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "488  xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "489  xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "490  xxs,xs,s,m,l,xl,xxl   women             205            104   \n",
       "491  xxs,xs,s,m,l,xl,xxl   women             188            238   \n",
       "492  xxs,xs,s,m,l,xl,xxl   women             205            173   \n",
       "493  xxs,xs,s,m,l,xl,xxl    kids             205            140   \n",
       "494  xxs,xs,s,m,l,xl,xxl   women             138             43   \n",
       "495  xxs,xs,s,m,l,xl,xxl   women              79            148   \n",
       "496          xs,s,m,l,xl  unisex             139             26   \n",
       "497  xxs,xs,s,m,l,xl,xxl   women             135            206   \n",
       "498  xxs,xs,s,m,l,xl,xxl   women             181            181   \n",
       "499  xxs,xs,s,m,l,xl,xxl     men             139            137   \n",
       "\n",
       "    rgb_b_main_col rgb_r_sec_col  rgb_g_sec_col  rgb_b_sec_col  label  \n",
       "0               57           255            187            255      0  \n",
       "1              104           255            187            255      0  \n",
       "2                0           255            187            255      0  \n",
       "3              149           164            211            238      0  \n",
       "4              226           164            211            238      0  \n",
       "5              205           164            211            238      1  \n",
       "6               26           205            155            155      0  \n",
       "7              250           205            155            155      1  \n",
       "8              181           205            155            155      0  \n",
       "9              137           205            155            155      1  \n",
       "10              57           255            187            255      0  \n",
       "11             104           255            187            255      0  \n",
       "12               0           255            187            255      0  \n",
       "13             149           164            211            238      1  \n",
       "14             226           164            211            238      0  \n",
       "15             205           164            211            238      1  \n",
       "16              26           205            155            155      0  \n",
       "17             250           205            155            155      1  \n",
       "18             181           205            155            155      0  \n",
       "19             137           205            155            155      0  \n",
       "20              57           255            187            255      0  \n",
       "21             104           255            187            255      1  \n",
       "22               0           255            187            255      0  \n",
       "23             149           164            211            238      1  \n",
       "24             226           164            211            238      0  \n",
       "25             205           164            211            238      0  \n",
       "26              26           205            155            155      0  \n",
       "27             250           205            155            155      0  \n",
       "28             181           205            155            155      0  \n",
       "29             137           205            155            155      0  \n",
       "..             ...           ...            ...            ...    ...  \n",
       "470             57           255            187            255      0  \n",
       "471            104           255            187            255      0  \n",
       "472              0           255            187            255      0  \n",
       "473            149           164            211            238      0  \n",
       "474            226           164            211            238      0  \n",
       "475            205           164            211            238      0  \n",
       "476             26           205            155            155      0  \n",
       "477            250           205            155            155      0  \n",
       "478            181           205            155            155      0  \n",
       "479            137           205            155            155      0  \n",
       "480             57           255            187            255      0  \n",
       "481            104           255            187            255      0  \n",
       "482              0           255            187            255      0  \n",
       "483            149           164            211            238      0  \n",
       "484            226           164            211            238      0  \n",
       "485            205           164            211            238      0  \n",
       "486             26           205            155            155      0  \n",
       "487            250           205            155            155      0  \n",
       "488            181           205            155            155      0  \n",
       "489            137           205            155            155      0  \n",
       "490             57           255            187            255      0  \n",
       "491            104           255            187            255      0  \n",
       "492              0           255            187            255      0  \n",
       "493            149           164            211            238      0  \n",
       "494            226           164            211            238      0  \n",
       "495            205           164            211            238      0  \n",
       "496             26           205            155            155      0  \n",
       "497            250           205            155            155      0  \n",
       "498            181           205            155            155      0  \n",
       "499            137           205            155            155      0  \n",
       "\n",
       "[500 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'article', 'sales', 'regular_price', 'current_price',\n",
       "       'ratio', 'retailweek', 'promo1', 'promo2', 'customer_id', 'article.1',\n",
       "       'productgroup', 'category', 'cost', 'style', 'sizes', 'gender',\n",
       "       'rgb_r_main_col', 'rgb_g_main_col', 'rgb_b_main_col', 'rgb_r_sec_col',\n",
       "       'rgb_g_sec_col', 'rgb_b_sec_col', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>cost</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.781800</td>\n",
       "      <td>52.391200</td>\n",
       "      <td>28.290800</td>\n",
       "      <td>0.545646</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>2721.726500</td>\n",
       "      <td>6.517000</td>\n",
       "      <td>161.400000</td>\n",
       "      <td>139.600000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>207.700000</td>\n",
       "      <td>181.400000</td>\n",
       "      <td>209.900000</td>\n",
       "      <td>0.13928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>87.934743</td>\n",
       "      <td>35.272128</td>\n",
       "      <td>22.578343</td>\n",
       "      <td>0.194363</td>\n",
       "      <td>0.240975</td>\n",
       "      <td>0.069829</td>\n",
       "      <td>1908.085499</td>\n",
       "      <td>3.914728</td>\n",
       "      <td>39.790147</td>\n",
       "      <td>63.641814</td>\n",
       "      <td>81.148727</td>\n",
       "      <td>35.313205</td>\n",
       "      <td>23.474359</td>\n",
       "      <td>45.306849</td>\n",
       "      <td>0.34624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.296482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>25.950000</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>40.950000</td>\n",
       "      <td>20.950000</td>\n",
       "      <td>0.525044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2091.000000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>79.950000</td>\n",
       "      <td>37.950000</td>\n",
       "      <td>0.699248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4570.250000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>898.000000</td>\n",
       "      <td>197.950000</td>\n",
       "      <td>195.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5999.000000</td>\n",
       "      <td>13.290000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sales  regular_price  current_price          ratio  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean       56.781800      52.391200      28.290800       0.545646   \n",
       "std        87.934743      35.272128      22.578343       0.194363   \n",
       "min         1.000000       3.950000       1.950000       0.296482   \n",
       "25%        10.000000      25.950000      11.950000       0.354839   \n",
       "50%        26.000000      40.950000      20.950000       0.525044   \n",
       "75%        64.000000      79.950000      37.950000       0.699248   \n",
       "max       898.000000     197.950000     195.950000       1.000000   \n",
       "\n",
       "              promo1         promo2    customer_id           cost  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.061900       0.004900    2721.726500       6.517000   \n",
       "std         0.240975       0.069829    1908.085499       3.914728   \n",
       "min         0.000000       0.000000       1.000000       1.290000   \n",
       "25%         0.000000       0.000000    1017.000000       2.290000   \n",
       "50%         0.000000       0.000000    2091.000000       6.950000   \n",
       "75%         0.000000       0.000000    4570.250000       9.600000   \n",
       "max         1.000000       1.000000    5999.000000      13.290000   \n",
       "\n",
       "       rgb_r_main_col  rgb_g_main_col  rgb_b_main_col  rgb_r_sec_col  \\\n",
       "count   100000.000000   100000.000000   100000.000000  100000.000000   \n",
       "mean       161.400000      139.600000      133.500000     207.700000   \n",
       "std         39.790147       63.641814       81.148727      35.313205   \n",
       "min         79.000000       26.000000        0.000000     164.000000   \n",
       "25%        138.000000      104.000000       57.000000     164.000000   \n",
       "50%        160.000000      144.000000      143.000000     205.000000   \n",
       "75%        205.000000      181.000000      205.000000     255.000000   \n",
       "max        205.000000      238.000000      250.000000     255.000000   \n",
       "\n",
       "       rgb_g_sec_col  rgb_b_sec_col         label  \n",
       "count  100000.000000  100000.000000  100000.00000  \n",
       "mean      181.400000     209.900000       0.13928  \n",
       "std        23.474359      45.306849       0.34624  \n",
       "min       155.000000     155.000000       0.00000  \n",
       "25%       155.000000     155.000000       0.00000  \n",
       "50%       187.000000     238.000000       0.00000  \n",
       "75%       211.000000     255.000000       0.00000  \n",
       "max       211.000000     255.000000       1.00000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 24 columns):\n",
      "country           100000 non-null object\n",
      "article           100000 non-null object\n",
      "sales             100000 non-null int64\n",
      "regular_price     100000 non-null float64\n",
      "current_price     100000 non-null float64\n",
      "ratio             100000 non-null float64\n",
      "retailweek        100000 non-null object\n",
      "promo1            100000 non-null int64\n",
      "promo2            100000 non-null int64\n",
      "customer_id       100000 non-null float64\n",
      "article.1         100000 non-null object\n",
      "productgroup      100000 non-null object\n",
      "category          100000 non-null object\n",
      "cost              100000 non-null float64\n",
      "style             100000 non-null object\n",
      "sizes             100000 non-null object\n",
      "gender            100000 non-null object\n",
      "rgb_r_main_col    100000 non-null int64\n",
      "rgb_g_main_col    100000 non-null int64\n",
      "rgb_b_main_col    100000 non-null int64\n",
      "rgb_r_sec_col     100000 non-null int64\n",
      "rgb_g_sec_col     100000 non-null int64\n",
      "rgb_b_sec_col     100000 non-null int64\n",
      "label             100000 non-null int64\n",
      "dtypes: float64(5), int64(10), object(9)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if there any null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    86072\n",
       "1    13928\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem here is Imbalanced Data set lead to biased to the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12042532048>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].plot(kind=\"density\", figsize=(10,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unisex</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>women</td>\n",
       "      <td>0.139286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kids</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>men</td>\n",
       "      <td>0.137600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender     label\n",
       "2  unisex  0.142600\n",
       "3   women  0.139286\n",
       "0    kids  0.137600\n",
       "1     men  0.137600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CostomerBuy = df[df['label']==1]['gender'].value_counts()\n",
    "CostomerNotBuy =df[df['label']==0]['gender'].value_counts()\n",
    "df1 = pd.DataFrame([CostomerBuy,CostomerNotBuy])\n",
    "df[[\"gender\", \"label\"]].groupby(['gender'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1203d302128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHuCAYAAAAvGyAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmUHlWd//F3L+lASJOE0JKgEIjIF5RhR5awKKDIMIiKEgHZxQ0HmQGGHVmMAyoqOIAcFgMICMf4E1BwA5TIiIooOiBXRgLIEsjEYCeBdNLL74+nEjoRzBO6O3Wf6vfrnJw8dZ9bT3+rT1L96XvrVjX19fUhSZKk/DSXXYAkSZJenUFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMtZZdwACMBHYAngN6Sq5FkiTpH2kBJgK/Brrq3amRg9oOwMyyi5AkSVoFuwE/r7dzIwe15wDmzVtIb29f2bWoAYwfP5q5cxeUXYakivHcono0NzcxbtxaUOSXejVyUOsB6O3tM6ipbv5bkTQUPLdoFazS5VouJpAkScqUQU2SJClTjTz1+Zp6erqZN28O3d2Lyy4la62tbYwb10FLSyX/GUiS1PAq+RN63rw5rLHGKNZaawJNTU1ll5Olvr4+Fi7sZN68Oay77sSyy5EkSa+iklOf3d2LWWuttQ1p/0BTUxNrrbW2o46SJGWskkENMKTVwe+RJEl5q2xQkyRJanSVvEZtRe1rr8kaIwf/UBd1dTO/8+VB/1xJkiQYJkFtjZGt7H/irYP+ubdfdADzB/1TJUmSaoZFUCvT4YdP5bzzLmCjjTbmnHPOYPTo0Zx00mn8z//8nmuvvZp/+qet+NGP7qS5uZkddtiJT33qeF544XlOO+0kJk2axKxZj7PpppuxxRZbcued32P+/E4+//kvsdFGG/PHPz7MJZd8ma6uRYwZM5aTTz6d9dd/I5/+9Md461vfxkMP/Y4XX5zHCSeczM47Tyn7WyFJklaR16gNsZ133pXf/OZXADz++P/y+9//DoBf/vIX7LLLbvz85/dy1VXXc801N/DMM3/hu9+dAcCf//wYhx56BNOn38Qf/vAQs2c/xxVXfIO9996H2277DkuWLOGCCz7HZz87jWuuuYEPf/gjXHjhtGVfd8mSbq644hv867/+O1deefnqP3BJkjRgjqgNsZ13nsLNN9/IttvuwMYbT+bJJ59k3ry/cv/997Hxxm9m7733YY011gBgv/3ey513fp9ddtmVddYZz6abbgZAR8cb2G67HQCYMGEiv/3ts/zlL0/y7LNPc+qp/77say1cuHDZ6x133BmAyZPfzPz5navrcCVJ0iAyqA2xLbbYkmnTzuWBB37FNttsx7hx47nnnrvo7u6mvb19ub59fbWnKgCMGDFiufdaWlqW2+7p6WX99d/I9Ok3Fts9zJv312Xvt7W1AbVbcPT1+bBgSZIakVOfQ6y1tZW3vvWtfPvb32KbbbZnu+2257rrrmGnnaaw7bY78JOf/JCurkV0d3dzxx23se2229f1uZMmbURnZycPPfRbAL7//ds455wzhvJQJEnSauaI2mqw88678rvfPcikSRuxzjrjmTfvr0yZshtbbLEljz2WOOaYw+np6ebtb9+JAw+cypw5L6z0M9va2jj//Au4+OIvsXjxYkaNWoszzzx3NRyNJElaXZoaeFpsI2DW3LkL6O1d/hhmz36SCRMmLdv2PmqvbcXvVZV1dLQzZ443VJFer3HtbbSuMbLsMtQguhd1MW++jylcqrm5ifHjRwNsDDxR737DYkRtfufL3u9MkgaodY2R3HfAgWWXoQYx5dYZYFAbMK9RkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScrUsFj1OW5MG61tg7+kvHtxF/P+5ooWSZI0NIZFUGttG8nj0wZ/SfnkM2YAgx/UjjzykGWPhpIkScOXU58ZMqRJkiQwqK0WDz74AJ/+9MeWbU+bdg533HE7Rx11COeffxaHHXYQn/nMJ+ns/BsAu+5ae97nAw/8iqOP/gjHHHMYJ5zwKV588UUA7rzzexx99KEceeQh/Od/nkdXVxcpPcp737sP8+bNo7Pzbxx44L/w2GNp9R+sJEkaNAa1Ev3v/z7G1KmHcv31tzB69Gh+9KM7l3v/2muv5uSTT+Pqq69nhx125E9/epTHH/8zt9/+XS6//BqmT7+RcePW4aabridiMw444ANcdtnFfOUrX+R97/sgb3lLlHRkkiRpMAyLa9RyNW7cOmy66WYATJ68CZ2dncu9v+uuu3P66Sez2257sNtue7DDDjsxY8bNPP30X/j4x48CoLt7ybLPOOKIYzjmmMMYOXIkZ5113uo9GEmSNOgMaqtBU1PTctvd3d0AtLW1Ldfe17f8w+WnTj2UKVN257//eyaXXXYJ73jHw6y55ij23HNvTjjhZABeeuklenp6AFiwYAEvvfQSL730Ep2dnYwdO3aoDkmSJK0GTn2uBmPGjOXZZ5+hq6uLzs6/8dBDv61rv2OPPYKXXlrIQQcdwkEHHcKf/vQo22yzHffe+1PmzfsrfX19XHTRf3LLLbXFBxdddAEHHvgh3v/+D3LRRRcM5SFJkqTVYFiMqHUv7ipupTH4n1uPyZPfzM47T+Gwww5i4sT12Wqrbera7+MfP45p086lpaWFUaNGccopZ7LBBhty1FHHcvzxn6Cvr49NNtmUj3zkSO6668c888zTnHPONPr6+vjoRw/nrrt+zF57vWsghyhJkkrUtOJ0WwPZCJg1d+4CenuXP4bZs59kwoRJpRTVaIbT96qjo505c+aXXYbUsDo62rnvgMG/J6WqacqtMzzn9tPc3MT48aMBNgaeqHu/oSpIkiRJA2NQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJV1+05IuIjwGnF5p0ppZMiYmvgKmBt4F7gEyml7ojYEPgm8AYgAYemlBZExFjgBmAyMAc4KKU0OyLagKuB7YGXgUNSSo8O3iFKkiQ1ppWOqEXEKOASYA9gK2C3iNibWhj7dEppU6AJOLbY5TLgspTSZsADwFlF++eAmSmlzYErgYuL9uOBhUX7CcD0QTiu5bSPHUlHR/ug/2kfO7Kur7/iQ9kBLrjgfB599JG/67v0ge2SJEn1jKi1UAt0awELgRHAEmDNlNL9RZ/pwLkRcRWwO/C+fu0/A04B9iveA7gJuDQiRhTtZwOklO6NiI6I2DCl9NTADu0Va4xo46CbPzlYH7fMLVMvZz713fR2RaeeetbKO0mSpGFtpUEtpTQ/Is4CHgVeoha8FgPP9ev2HPAmYF2gM6XUvUI7wPpL9ymmSDuBjv7tK+xTV1Arbh63nBdeaKa1dfVcflfP12lpaaapqYnW1mZuvvlGfvrTe+jqWsRxx32Gbbfdjosv/jL33TeTddftoLe3h+22256urpc466zTmTv3/wA45piPs/vuewx6/c3NzXR0tA/65+ZqOB2rJJXNc+7ArTSoRcSWwNHAJOBv1KY83w30fxxAE9BLbeRtxUcd9Pbr099r7dPUb5+VerUnE/T29tLdXfdHDEg9X6enp5e+vj5uu+1W7r77Lr74xYv5j/84gZ6eXn7ykx+T0qNcf/0tzJ8/nyOP/DC9vX3cfffdrLfeRL7wha/y2GOJH/3oB+yyy26DXn9vb++wuXO0TyaQBsYfulpVnnNf0e/JBKu2Xx199gHuSim9kFLqojad+Q5gYr8+E4BngReAMRHRUrRPLNoBnin6ERGtQDswF3j6NT6rUmbN+jMXXvg5PvShDzNq1Khl7b/97W/YY4930trayrhx49hppykAbLHFlsyc+VNOO+1E/vjHRzjyyGPKKl2SJJWknqD2ELB3RKwVEU3A/tSmPxdFxJSiz2HUVoMuAWYCU4v2w4E7i9d3FNsU788s+i9rj4hdgUWDeX1aLkaNWotp077IpZdewssvv7ysvampif6PW21pqWXcDTbYkBtv/Dbvete+PPTQbzn22CPo7V09o4SSJCkPKw1qKaUfUbv4/zfA76ktJrgAOBT4SkQ8CoymtjIU4FPAxyLiEWA34Myi/Sxgp4h4uOhzXNH+NWBk0X4JtdBXOeutN4Fdd92dbbbZlquu+vqy9u23fzt33/1jFi9eTGdnJ7/85S8AmDHjZq6++gr23HNvTjzxVObNm8fChQvLKl+SJJWgrvuopZQuBC5cofkh4O2v0vdJalOjK7b/FXjvq7QvAo6op44qOO64z3DYYVNZtGgRALvt9g7++MdHOPzwqayzzng22mgyAO95z36cc84ZHH74VFpaWjjuuONpb/f6EEmShpOmvr4Vr/1vGBsBs15tMcHs2U8yYcKkZdvtY0eyxoi2QS9g0ZLFzH/x9d2eIxcrfq+qzMUE0sB0dLRz3wEHll2GGsSUW2d4zu2n32KCjYEn6t2vrhG1Rjf/xa7Xfb8zSZKksvisT0mSpEwZ1CRJkjJlUJMkScrUsLhGTZI0cD1di5ly64yyy1CD6OlaXHYJlWBQkyTVpWVkGwfd/Mmyy1CDuGXq5eBCvgEbFkFtXHsbrWuMHPTP7V7Uxbz5/sYgSZKGxrAIaq1rjBySe/9MuXUGGNQkSdIQcTGBJElSpobFiFrZHnzwAa677hpGjBjBc889y5Qpu7Pmmmsyc+bP6Ovr40tfupg//Slx9dVfp7u7m4kT38gpp5zBmDFj+eAH92efff6ZX/3qF7z88iLOPPNcNtts87IPSZIkrQaOqK0mjzzyMCeddBpXXXU93/nOLYwdO46rr76eTTZ5C9/97gy+/vX/4qKL/otvfONG3v72nbj88q8t23fMmDFceeV1vO99H+D6668p8SgkSdLq5IjaajJ58ptZb70JAIwZM5btt689z3699SZw330zef752Rx//CcA6O3tYe21xyzbd8cddyk+YxN+9rN7VnPlkiSpLAa11aS1dflvdUtLy7LXvb09bLnlVlx44VcA6Orq4uWXX172flvbKw+U7+tb/gH0kiSpupz6zMBb37oFDz/8B5566kkApk+/iksv/WrJVUmSpLINixG17kVdQ3I37e5Fg3Mjv3XWGc+pp57N2WefRm9vDx0d63H22ecNymdLkqTG1dTAU2kbAbPmzl1Ab+/yxzB79pNMmDCplKIazXD6XnV0tDNnzvyyy5AaVkdHu08mUN1umXq559x+mpubGD9+NMDGwBN17zdUBUmSJGlgDGqSJEmZqmxQa+Ap3dXG75EkSXmrZFBrbW1j4cJOg8g/0NfXx8KFnbS2tq28syRJKkUlV32OG9fBvHlzWLDgxbJLyVpraxvjxnWUXYYkSXoNlQxqLS2trLvuxLLLkCRJGpBKTn1KkiRVgUFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVOtZRcgSWoMi7sXc8vUy8suQw1icffiskuoBIOaJKkuba1tPD7twLLLUIOYfMYMoKvsMhqeU5+SJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqZa6+kUEfsDnwXWAn6UUvpMROwNfBlYE7g5pXRm0Xdr4CpgbeBe4BMppe6I2BD4JvAGIAGHppQWRMRY4AZgMjAHOCilNHswD1KSJKkRrXRELSImA18H3gdsCWwbEfsC1wAHAJsDOxRtUAtjn04pbQo0AccW7ZcBl6WUNgMeAM4q2j8HzEwpbQ5cCVw8GAcmSZLU6OoZUXs/tRGzpwEiYirwFuCxlNKsou2bwIci4hFgzZTS/cW+04FzI+IqYHdqYW9p+8+AU4D9ivcAbgIujYgRKaUlAzw2SdIg6l2ymMlnzCi7DDWI3iWLyy6hEuoJapsAiyPiNmBD4HvAw8Bz/fo8B7wJWP812tcFOlNK3Su003+fYoq0E+gAnn09ByRJGhrNI9rY/8Rbyy5DDeL2iw4Ausouo+HVE9RaqY14vQNYANwGvAz09evTBPRSm0qtp52ifWmf/pr6vbdS48ePrrerREdHe9klSNKw4Tl34OoJarOBn6SU5gBExP8DPgT09OszgdoI2NPAxFdpfwEYExEtKaWeos/SEbNnin5PR0Qr0A7MrfcA5s5dQG/vihlQ+nsdHe3MmTO/7DKkhuUPXa0qz7mvaG5uel2DS/XcnuN7wD4RMTYiWoB9gW8DERGbFG2HAHemlJ4EFkXElGLfw4r2JcBMYGrRfjhwZ/H6jmKb4v2ZXp8mSZJUR1BLKf0S+ALwc+AR4EngcuBIYEbR9ii18AZwKPCViHgUGA1cUrR/CvhYseBgN+DMov0sYKeIeLjoc9yAj0qSJKkCmvr6GnbacCNgllOfqpdTn9LAdHS0u5hAdbv9ogM85/bTb+pzY+CJuvcbqoIkSZI0MAY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMtVab8eI+BKwbkrpyIjYGrgKWBu4F/hESqk7IjYEvgm8AUjAoSmlBRExFrgBmAzMAQ5KKc2OiDbgamB74GXgkJTSo4N4fJIkSQ2rrhG1iNgLOKJf0zeBT6eUNgWagGOL9suAy1JKmwEPAGcV7Z8DZqaUNgeuBC4u2o8HFhbtJwDTX/+hSJIkVctKg1pErANMAz5fbE8C1kwp3V90mQ58KCJGALsD3+7fXrzej9qIGsBNwL5F/2XtKaV7gY5iVE6SJGnYq2dE7QrgDGBesb0+8Fy/958D3gSsC3SmlLpXaF9un+L9TqDjH3yWJEnSsPcPr1GLiI8Cf0kp3RURRxbNzUBfv25NQO+rtFO0L+3T32vt09Rvn7qMHz96VbprmOvoaC+7BEkaNjznDtzKFhNMBSZGxO+AdYDR1ILVxH59JgDPAi8AYyKiJaXUU/R5tujzTNHv6YhoBdqBucDTRb8/r/BZdZs7dwG9vSvmQ+nvdXS0M2fO/LLLkBqWP3S1qjznvqK5uel1DS79w6nPlNK7UkpbpJS2Bs4GbkspHQUsiogpRbfDgDtTSkuAmdTCHcDhwJ3F6zuKbYr3Zxb9l7VHxK7AopTSU6t8FJIkSRVU9+05VnAocGVErA08CFxStH8KuDYizgSeAg4u2s8CpkfEw8CLxf4AXwOuKNq7qIU+SZIkAU19fQ07bbgRMMupT9XLqU9pYDo62tn/xFvLLkMN4vaLDvCc20+/qc+NgSfq3m+oCpIkSdLAGNQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpU631dIqIzwIHFZvfTyn9R0TsDXwZWBO4OaV0ZtF3a+AqYG3gXuATKaXuiNgQ+CbwBiABh6aUFkTEWOAGYDIwBzgopTR70I5QkiSpQa10RK0IZO8GtgG2BraLiIOBa4ADgM2BHSJi32KXbwKfTiltCjQBxxbtlwGXpZQ2Ax4AziraPwfMTCltDlwJXDwYByZJktTo6pn6fA44MaW0OKW0BPgjsCnwWEppVkqpm1o4+1BETALWTCndX+w7vWgfAewOfLt/e/F6P2ojagA3AfsW/SVJkoa1lU59ppQeXvo6It5CbQr0a9QC3FLPAW8C1n+N9nWBziLU9W+n/z7FFGkn0AE8W88BjB8/up5uEgAdHe1llyBJw4bn3IGr6xo1gIh4G/B94GSgm9qo2lJNQC+1Ebq+Otop2pf26a+p33srNXfuAnp7V/xo6e91dLQzZ878ssuQGpY/dLWqPOe+orm56XUNLtW16jMipgB3AaemlK4FngYm9usygdoI2Gu1vwCMiYiWon0ir4yYPVP0IyJagXZg7iofiSRJUsXUs5hgA+C7wCEppW8Vzb+svRWbFOHrEODOlNKTwKIi2AEcVrQvAWYCU4v2w4E7i9d3FNsU788s+kuSJA1r9Ux9ngSsAXw5Ipa2fR04EphRvHcHrywUOBS4MiLWBh4ELinaPwVcGxFnAk8BBxftZwHTI+Jh4MVif0mSpGGvqa+vYa/v2giY5TVqqpfXqEkD09HRzv4n3lp2GWoQt190gOfcfvpdo7Yx8ETd+w1VQZIkSRoYg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZaq17AI0+Ma1t9G6xsiyy8hSR0d72SVkp3tRF/PmLy67DEnSqzCoVVDrGiO574ADyy5DDWLKrTPAoCZJWXLqU5IkKVMGNUmSpEwZ1CRJkjLlNWoV1NO1uHbdkVSHni6vT5OkXBnUKqhlZBsH3fzJsstQg7hl6uVAV9llSJJehVOfkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqZcTFBBi7sXFxeISyu3uNtVn5KUK4NaBbW1tvH4NB8hpfpMPmMGrvqUpDw59SlJkpQpg5okSVKmDGqSJEmZMqhJkiRlysUEFdS7ZHFxgbi0cr1LXPUpSbkyqFVQ84g29j/x1rLLUIO4/aIDcNWnJOXJqU9JkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTrWUXABARhwBnAiOAr6aULi25JEmSpNKVPqIWEW8EpgG7AlsDH4uIt5ZblSRJUvlyGFHbG7g7pfRXgIj4NvBB4LyV7NcC0NzcNLTVNag3jFuz7BLUQPx/pHp5btGq8Nzyin7fi5ZV2S+HoLY+8Fy/7eeAt9ex30SAcePWGoqaGt7VZ7677BLUQMaPH112CWoQnlu0Kjy3vKqJwJ/r7ZxDUGsG+vptNwG9dez3a2A3asGuZwjqkiRJGiwt1ELar1dlpxyC2tPUAtdSE4Bn69ivC/j5kFQkSZI0+OoeSVsqh6D2E+CciOgAFgIHAh8rtyRJkqTylb7qM6X0DHAGcA/wO+DGlNKvyq1KkiSpfE19fX0r7yVJkqTVrvQRNUmSJL06g5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqqqyIuDQidii7DknVEhEPR8TJETGh7FpUfd5HTZUVEUcAhwNvAK4Drk8pzS63KkmNLiImUTu3HAI8DnwDuDWltKTUwlRJBjVVXkRsABwMfAJ4BLgqpfTdcquSVAUR8X7gEmAUcD1wfkppbrlVqUoMaqq0iNgY+Ai1oPY08C1gb6A7pXR4mbVJakwRMRr4IHAY8EZqI/bfAt4DHJ1S2r7E8lQxOTyUXRoSEfFzYAK1k+h7UkpPFe3XAc+UWZukhjYL+B5wbkrp3qWNEXE58K7SqlIlOaKmyoqIPVNKd5ddh6RqiYj2lNL8suvQ8GBQU2VFxDeAv/sHnlI6uoRyJFVERMzi1c8tk0uXjcI5AAAJz0lEQVQoRxXn1Keq7Kf9Xo8A3gs8Wk4pkirkHf1ejwDeD4wspxRVnSNqGjYiogm4L6W0S9m1SKqWiHjARQQaCo6oaTjZHJhYdhGSGltE7N5vswl4G7BmSeWo4gxqqqyI6KV2HUlT0TQHOLW8iiRVxLn9XvcB/wccUVItqjinPiVJkjLliJoqKSLagUUppSURMRWYAvwmpXRtyaVJamAR8R7gBeAP1J5IMAX4DXBKSumFMmtTNTmipsqJiIOAK4H5wFXUnsf3PWon1P9OKf1bieVJalARMY3ak03agOeBhcC1wJ7AximlA0osTxXliJqq6GxgU2Btar/1TkopPR8RI6n95itJr8f7gC2pPdfzL8C6KaVu4LaIeKjUylRZzWUXIA2B7pTS8ymlx4DHUkrPA6SUuoDF5ZYmqYEtSSn1FE8leKIIaUv1lFWUqs2gpirq7fd6SWlVSKqa3td4LQ0Zpz5VRZMi4hpqt+VY+ppie8PyypLU4N4WEY9TO5esX7ym2PYejRoSBjVV0b/3e/3TFd5bcVuS6rVp2QVo+HHVpyorIn6YUtqn7DokVUtEzEgpHbhC210ppb3KqknV5YiaqmzNiNggpfSXsguR1Pgi4jvA1iw/7Qm1B7M/VU5VqjqDmqqsA3giIl4AXqZ2HUlfSmlyuWVJalBHAusAFwPH92vvpnZfNWnQGdRUZe8puwBJ1ZFS6gQ6gQMiYl9gL2o/R+9OKd1WanGqLG/PocpKKT1J7WkEH6P2QPY9ijZJet0i4mTgHGrTnbOAMyPijFKLUmW5mECVFREXAG8CtgN2BG4FHkwpnVhqYZIaWkT8HtgxpfRysT2K2rOENy+3MlWRI2qqsn2Aw6g9nL0TeBewb7klSaqA5qUhrbCI2nVq0qDzGjVV2dI7hy8dNh6JdxOXNHB3RcQMYHqxfQRwd3nlqMoMaqqyW4CbgXUi4gRqo2s3lluSpAo4AfgkcDi1mam7gStKrUiV5TVqqrSI2AfYG2ihtjLreyWXJElS3QxqqrSI+CdgXP+2lNK9JZUjqYFFRC+vXErxd1JKLauxHA0TTn2qsiLiW8C2wDP9mvuAPcupSFIjSykttwAvIpqBU4B/A04vpShVnkFNVbYVsHlKqafsQiRVS0RsTm0xwTxgOx9Vp6FiUFOV/RLYBEhlFyKpGiKiCTiV2ijaGSmlK0suSRVnUFOV3QU8HBHPUrvHkc/6lPS6rTCKtm1K6elyK9JwYFBTlZ1O7Xo0HxslaTD8tvj7F8B1EbHcmyklr3/VoDOoqcr+D5iZUnJps6TBsE/ZBWj48fYcqqyIuBrYAvgxsHhpe0rpvNKKktTwIuKHKSVDm1YLn/WpKnsKuANYQu36tKV/JGkgRkXEBmUXoeHBETVVWkR0ADtSm+b/RUrp+ZJLktTgIuKPwKbAC8DLuFBJQ8hr1FRZxeOjrgHupzZ6fEVEHONjpCQN0HvKLkDDh1OfqrJpwK4ppQNTSu8HdgY+V3JNkhpcSulJYArwMWAOsEfRJg06g5qqbERKadbSjZTS4/hvXtIARcQFwD8DH6A2M3VURFxUblWqKqc+VWVPRcQJwNXF9kfxnmqSBm4fas8RfjCl1BkR7wJ+D5xYblmqIkcXVGXHUJvufByYVbw+ttSKJFVBb/H30tV4I/u1SYPKETVV2VYppan9GyLiA8B3SqpHUjXcAtwMrFOM2h8G3FhuSaoqb8+hyomIqdR+wz0POLvfW63A6SmlTUopTFJlFKvK9wZagLtdTa6h4oiaqqid2oqsduCd/dq7gTNKqUhS1TwL3L50IyJ2TyndW2I9qihH1FRZEbFXSumufttrp5Q6y6xJUuOLiG9RW0zwTL/mPh/KrqHgiJqqbFREXAicD/wa6IiIk1JK08stS1KD2wrYPKXUU3Yhqj5XfarKzqZ2ge+HgV8BGwH/WmZBkirhl4DXumq1MKip0lJKDwH7AbellBYAI0ouSVLjuwt4OCKeiojHI2JWRDxedlGqJqc+VWXPR8TXgB2AjxR3Dn+q5JokNb7TgT3xBtpaDQxqqrKDgfcDX00pLSx+4z2n3JIkVcD/ATNTSq7G05AzqKnKFgCjgQsjohW4B1hYbkmSKuBPwP0R8WNg8dLGlNJ55ZWkqjKoqcq+ALwFuAZoAo4CJgOfKbMoSQ3vKV65jKKpzEJUfd5HTZUVEQ8B26SUeovtVuAPKaXNy61MUqOLiA5gR2oDHr9IKT1fckmqKFd9qspaWX6VZyvgfY8kDUjx+KjfURulPwL4fUT8S7lVqaqc+lSV3QDcExE3FdsH44OTJQ3cNGDXlNIsgIiYDHwH8HmfGnSOqKmSImIccCW1B7NvCBwJXJ5S+nyZdUmqhBFLQxpASulx/HmqIeKImionIrYB7gCOSin9APhBRHweuCAiHkop/b7cCiU1uKci4gTg6mL7o3hPNQ0RfwNQFX0JOLgIaQCklE4Hjga+XFpVkqriGGBn4HFgVvH62FIrUmU5oqYqGpdS+umKjSmlHxYPaZekgdgqpTS1f0NEfIDadWrSoDKoqYpGRETz0ttyLBURzUBbSTVJanARMRUYCZwXEWf3e6uV2mOlDGoadAY1VdHPgM8Wf/o7E3hg9ZcjqSLagSnF3+/s194NnFFKRao8b3iryomIdmqLCTagdq+jRcC2wAvAe1NKfy2xPEkNLiL2Sind1W977ZRSZ5k1qboMaqqkiGii9hvvNkAv8EBKaWa5VUmqgojYH9gVOB/4NdABnJRSml5mXaomg5okSasgIn5N7ZYcOwC7AccBP0spbVdqYaokb88hSdIqSik9BOwH3JZSWsDyj6uTBo1BTZKkVfN8RHyN2ojaDyLiIuCpkmtSRRnUJElaNQdTuzZtj5TSQmo3vj243JJUVd6eQ5KkVbMAGA1cGBGtwD3AwnJLUlUZ1CRJWjVfAN4CXAM0AUcBk4HPlFmUqsmgJknSqnk3sM3Sp59ExPeBP5RbkqrKa9QkSVo1rSy/yrMV6CmpFlWcI2qSJK2aG4B7IuKmYvtg4MYS61GFecNbSZLqFBHjqA1ybAfsBewJfDWldH2phamynPqUJKkOEbEN8AiwXUrpBymlk4EfAhdExJblVqeqMqhJklSfLwEHp5R+sLQhpXQ6cDTw5dKqUqUZ1CRJqs+4lNJPV2xMKf0QWHf1l6PhwKAmSVJ9RkTE3/3cLNraSqhHw4BBTZKk+vwM+OyrtJ8JPLCaa9Ew4apPSZLqEBHtwB3ABsDvgEXAtsALwHtTSn8tsTxVlEFNkqQ6RUQT8E5gG6AXeCClNLPcqlRlBjVJkqRMeY2aJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqb+Pzf5bGG5gUANAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.index = ['CostomerBuy','CostomerNotBuy']\n",
    "df1.plot(kind='bar',stacked=True, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Austria</td>\n",
       "      <td>0.143540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>0.138401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>0.132406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country     label\n",
       "0  Austria  0.143540\n",
       "2  Germany  0.138401\n",
       "1   France  0.132406"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CostomerBuy = df[df['label']==1]['country'].value_counts()\n",
    "CostomerNotBuy =df[df['label']==0]['country'].value_counts()\n",
    "df2 = pd.DataFrame([CostomerBuy,CostomerNotBuy])\n",
    "df[[\"country\", \"label\"]].groupby(['country'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12042c5f1d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAHuCAYAAAAvGyAsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XucVVX9//HXDMNFYVDSUbAU5Wt+vJTXNE2zvKVkaqVJ3rXUtMwu2tVLllppUamVmqJ4TS3KS+ElrYQu9s0s7We5fv0ERW6ChHFJGGbm/P44Bx0Q9SADe509r+fjwYOz11n7zGePuOc9a+21d1OlUkGSJEn5aS66AEmSJK2YQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEy1FF3AKugP7AzMADoLrkWSJOnV9AGGAX8GFte7UyMHtZ2BiUUXIUmStBLeCfyu3s6NHNRmAMydu5CurkrRtagBrLfeIObMWVB0GZJKxnOL6tHc3MSQIQOhll/q1chBrROgq6tiUFPd/LciaXXw3KKVsFKXa7mYQJIkKVMGNUmSpEw18tTnK+rs7GDu3Nl0dLQXXUoptLT0Y8iQNvr0KeU/F0mSslXKn7xz585mwIC1GThwKE1NTUWX09AqlQoLF85j7tzZrL/+sKLLkSSpVynl1GdHRzsDBw42pPWApqYmBg4c7OikJEkFKGVQAwxpPcjvpSRJxShtUJMkSWp0pbxGbXmtg9diQP+eP9RFizuYP++Fuvp2dHRw003Xcd99d9PU1ERnZycjR76PY445wRErSZK0Qr0iqA3o38JBZ9zR45971+hDmF9n39GjL2Lu3DlcccW1tLa2snDhAr785c8xcOAgDj308B6vTZIkNb5eEdSKNmvWs9x333h+/vO7aW1tBWDgwEF89rNfYPLkJ/n3v+fwrW99nWeffZbm5mY+9rFPsPPOb2fMmCt5/PH/w6xZMzn00FE88MB9RGzJY4/9jfb2dk455ZP85Ce38NRTkxg16khGjTqK2bNn8Y1vnM+CBfN57rnZvPe9B3Hiiacwfvxd/OlPf2DevHlMnz6NnXfelTPP/CLnn38O2223Iwcf/AEATjvtZE499XS22eYtRX7LJEkSBrU14p//fJxNNx3B4MGDl2kfPnxThg/flK985UsceODB7LHHu3juuef4+Mc/ytixNwPQ3r6YG2/8CQAPPHAflUqFq666nmuu+RHf+963uO66W3j++bkcf3w1qP3qV/ey3377M3Lk+1iwYAEf/OCBHHbYhwH4+98f48Ybb6O5uQ9HHnkoTz55GAceeAhjxlzJwQd/gJkzZ/D8888b0iRJyoRBbQ3pfh3ab35zP9dddw1dXZ3069efGTOm8fTTT3P11VcC1evZpk2bCsDWWy8bmnbddXcAhg4dxjbbvJUBAwYwdOgwFiyoTsIeeeQxPPLIw9x88w1MnvwkHR1LWLSoeh3dW9+6LWuvPRCAjTZ6I/Pm/YcddtiJ556bzYwZ07n33vEccMB7V+83QpIk1c2gtgZEbM1TT01i4cIFDBw4iL322pe99tqXGTOm88lPfozOzi4uvfRyBg9eB4DnnnuOIUOGMGHCb+nfv/8yn9XS8tJ/sj59+rzsa1122XeZPn0a++13AHvu+W4efvh/qVSqDwvu12/Zz6pUKjQ1NTFy5Pu4//57eeCB+/jud3/Q04cvSZJeJ2/PsQYMHTqU/fd/LxdccB7z51dHvjo6OvjDHybS3NzMTju9jZ/9rDq9OXnyJI49dhSLFy96XV/r4Yf/xJFHHsPee+/LlClPM3v2LLq6ul51n5Ej38ftt49jww2Hsv76ba/r60qSpJ7niNoacsYZX+SWW27i9NM/RldXF//973/ZYYed+Pa3L2Xttdfm4osv5LjjPkylUuGcc7724hTlyjr66OM5//xz6d+/PxtsMJQtt9ya6dOnveo+G244lA03HMrIkQe9rq8pSZJWj6al02INaFNg8pw5C+jqWvYYZs58mqFDh7+4ncN91HJVqVSYM+c5TjvtZK6//lb69eu3wn7Lf08bUVtbK7Nn13tDFUnLa123PwP6rvgcIS1v0ZJ25j+/uOgystHc3MR66w0C2Ax4qt79esWI2vx5L9R9v7Pe5re/fYDRo7/JGWd88RVDmiQBDOjbj8NvPbXoMtQgbht1OfMxqK2qXhHU9MqWLmyQJEn5cTGBJElSpgxqkiRJmTKoSZIkZcqgJkmSlKlesZhgyDr9aFnurvw9oaN9MXP/097jnytJkgS9JKi19OvPpAsP7fHPHXHWOKD+oDZp0v/j2GM/zAUXXMS7373PSn+96dOncd11Y/jSl8592Xu33/5TAN7//sNW+nMlSVKeekVQy8Uvf3kne+21L3fc8bPXFdRmzpzx4sPal2dAkySpfAxqa0hHRwf33XcPP/jBVZx66keYNm0qb3zjmzjssIO47LIrGTZsIx555GGuueZHfP/7P+KWW27k7rt/SXNzE1tttQ2f//xZXHLJt5k+fRqjR1/EXnvtw+WXX0pnZxcjRvwPw4ZtBMBHP/oxxo27lXvuGc+iRS/Qt29fzjvvQjbZZNNivwGSJGmluZhgDfnDH37H0KFD2WST4bzzne/mjjt+9op9Ozs7ufHGsYwZcwNjxtxIR0cHs2fP4lOfOpOIrTjjjC8A8MwzU7j00is4++yvvrjvwoULmDDhQb7//Su54YbbeMc73sm4cbet9uOTJEk9z6C2howffyf77rs/APvssx/jx9/FkiVLVti3T58+vOUt23Liicdy7bVX8eEPH0Vb2wYv67fxxsMZNGjQMm0DBw7ivPMu4P777+OKK77P738/gRdeaOznkUqS1Fs59bkGzJ37bx566A+k9AQ/+cktVCoV5s+fx4MP/pqmpiYqlepD5Ts7O17c5xvfGM3jj/+dhx76A2eccTrnnnv+yz63f/+Xr2R99tmZfPKTH+PQQw9n113fwRvesB7/+ldafQcnSZJWG4PaGnDPPePZaaddGD360hfbxoy5kttvH8c666zL5MmT2GijNzJx4oMAzJ07l9NOO4mrrrqet7xlW2bNepYnn/wXb37zlnR2dr7q13riiX/wpjdtzKhRR7F48SKuvvoKNtxww9V6fJIkafXoFUGto31x7VYaPf+59bj77rs4+eRPLNP2wQ8ezs03X88nPvFpLrnk21x77VXsssuuAAwZMoSDD/4AJ510LP37D2CTTYZz4IGH0N6+mAUL5nP++edw4IGHrPBr7bzzrvz85z/l6KM/RKVSYfvtd2TSpCdX7UAlSVIhmpZOuzWgTYHJc+YsoKtr2WOYOfNphg4dXkhRZVWG72lbWyuzZ88vugypYbW1tXL4racWXYYaxG2jLvec201zcxPrrTcIYDPgqbr3W10FSZIkadUY1CRJkjJlUJMkScqUQU2SJClTBjVJkqRM1XV7jog4GvhSbfPulNKZEbE9cDUwGJgAnJJS6oiITYAbgQ2ABByVUloQEesCNwEjgNnA4SmlmRHRDxgDvA14ATgypfREzx2iJElSY3rNoBYRawOXAlsAzwO/j4h9ge8BJ6aUHoqIMcBJwOXAD4EfppRuiYhzgHOALwAXABNTSgdGxDHAJcAo4HRgYUppq4jYExgL7NqTB9m6bn8G9O3Xkx8JwKIl7cx//rXvpTZjxnSOOOKDbLrpiGXaL7roO2y44dAer0uSJJVDPSNqfahOkQ4EFgJ9gSXAWimlh2p9xgJfjYirgT2B93drf5BqUDuw9h7Aj4EfRETfWvu5ACmlCRHRFhGbpJSmrNqhvWRA336r5d4/t426nPnUd9Pb9ddvY+zYm3u8BkmSVF6vGdRSSvNrI2NPAP+lGrzagRndus0A3gSsD8xLKXUs1w6w0dJ9alOk84C27u3L7VNXUKvdPG4Zs2Y109KyZi6/q+fr9OnTvMK+X/vaV/jPf55n6tRnOO20T9He3s7NN9/A4sWLWbJkCWed9RW23XY7Tj31JLbeehseffSvPP/8XD772S/wjnfszowZ07nggvOYO3cuAwYM4EtfOoc3v3kLxo//BbfccjOVShdbbrkVZ575xRU+F3RlNDc309bWukqfkYMyHIMkNQrPuauunqnPbYGPAMOB/1C9/uw9QPfHATQBXVRH3pZ/1EFXtz7dvdI+Td32eU0rejJBV1cXHR11f8QqqefrdHZ28dxzszn66A+/2Pae9xxApVJh8OB1uOmm79LV1cVnPvMJLrroe6y77rr84hd3MHbsNVx88XepVCq0ty/hiiuu5Xe/m8AVV/yAXXbZjYsv/gZ77rk3hx56OH/84++45pqrOeGEk7j99p9x+eVj6N+/P1dc8X1uuOE6jj/+xFU6zq6uroa/w7RPJpBWjT90tbI8576k25MJVko9U5/7Aw+klGYBRMRY4ExgWLc+Q4HpwCxgnYjok1LqrPWZXuszrdZvakS0AK3AHGBqrd+Ty31Wqaxo6vPCC89j663fAlRHrL7+9W/x+99PZMqUp/nrX/9Cc/NLI3Bvf/tuAIwY8T/Mnz8PgL/97RHOO+9CAHbbbQ92220Pxo27lalTn+FjHzsBgI6OJWyxxZar/fgkSVLPq2d+8FFg34gYGBFNwEFUpz8XRcTutT7HUF0NugSYSHWRAMCxwN211+Nr29Ten1jr/2J7ROwBLOrJ69Nyt3RK8r///S8nnXQc06dPY7vtduCww0bR/Tms/fpVF0M0NTW92N6nz0s5u1KpMHnyJDo7u9h7730ZO/Zmxo69mR/96Do+85nPr8EjkiRJPeU1g1pK6T6qF///BXiM6mKCbwJHAd+NiCeAQVRXhgJ8HDg5Iv4BvBM4u9Z+DrBrRDxe6/OJWvtlQP9a+6VUQ1+v88wzU2hqauLYYz/Cjju+jQcf/A1dXa8+rbr99jtw//33AfDww3/i4osvZIcddmLChN8yd+6/qVQqjB79DW67zUUMkiQ1orruo5ZSugi4aLnmR4FdVtD3aeDdK2j/N3DwCtoXAcfVU0eZbb75m9l88y048sjDaG5uYpddduOxx/72qvt85jOf56KLLuDnP/8pAwYM4AtfOJvNNhvBCSecxOmnn0KlUmHzzbfg6KOPXzMHIUmSelRT9+m1BrMpMHlFiwlmznyaoUOHv7hd9H3UymD572kjcjGBtGra2lpXy62OVE63jbrcc2433RYTbAY8Ve9+dY2oNbr5zy+u+35nkiRJufBZn5IkSZkyqEmSJGWqtEGtga+9y47fS0mSilHKoNbS0o+FC+cZMHpApVJh4cJ5tLT0/GIMSZL06kq5mGDIkDbmzp3NggXPF11KKbS09GPIkLaiy5AkqdcpZVDr06eF9dcf9todJUmSMlbKqU9JkqQyMKhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmWopugBJUmNo72jntlGXF12GGkR7R3vRJZSCQU2SVJd+Lf2YdOGhRZehBjHirHHA4qLLaHhOfUqSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZlqqadTRBwEfAUYCNyXUvpUROwLfAdYC7g1pXR2re/2wNXAYGACcEpKqSMiNgFuBDYAEnBUSmlBRKwL3ASMAGYDh6eUZvbkQUqSJDWi1xxRi4gRwBXA+4FtgR0jYiRwDXAIsBWwc60NqmHstJTSFkATcFKt/YfAD1NKWwIPA+fU2i8AJqaUtgKuAi7piQOTJElqdPVMfX6A6ojZ1JTSEmAU8F/gXymlySmlDqrh7EMRMRxYK6X0UG3fsbX2vsCewE+7t9deH0h1RA3gx8DIWn9JkqRerZ6pz82B9oi4E9gE+AXwODCjW58ZwJuAjV6hfX1gXi3UdW+n+z61KdJ5QBsw/fUckCRJUlnUE9RaqI6GvRtYANwJvABUuvVpArqojtDV006tfWmf7pq6vfea1ltvUL1dJdraWosuQZJ6Dc+5q66eoDYTuD+lNBsgIn5Oddqys1ufoVRHwKYCw1bQPgtYJyL6pJQ6a32WjphNq/WbGhEtQCswp94DmDNnAV1dy2dA6eXa2lqZPXt+0WVIDcsfulpZnnNf0tzc9LoGl+q5Ru0XwP4RsW5E9AFGUr3WLCJi81rbkcDdKaWngUURsXtt32Nq7UuAiVSvbwM4Fri79np8bZva+xNr/SVJknq11wxqKaU/ARcDvwP+ATwNXA4cD4yrtT3BSwsFjgK+GxFPAIOAS2vtHwdOjoh/AO8Ezq61nwPsGhGP1/p8YpWPSpIkqQSaKpWGnTbcFJjs1Kfq5dSntGra2lqZdOGhRZehBjHirHGec7vpNvW5GfBU3futroIkSZK0agxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpSplqILkCQ1hq4l7Yw4a1zRZahBdC1pL7qEUjCoSZLq0ty3HwedcUfRZahB3DX6EGBx0WU0PKc+JUmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJV96rPiPg2sH5K6fiI2B64GhgMTABOSSl1RMQmwI3ABkACjkopLYiIdYGbgBHAbODwlNLMiOgHjAHeBrwAHJlSeqIHj0+SJKlh1TWiFhH7AMd1a7oROC2ltAXQBJxUa/8h8MOU0pbAw8A5tfYLgIkppa2Aq4BLau2nAwtr7Z8Gxr7+Q5EkSSqX1wxqEfEG4ELg67Xt4cBaKaWHal3GAh+KiL7AnsBPu7fXXh9IdUQN4MfAyFr/F9tTShOAttqonCRJUq9Xz4jalcBZwNza9kbAjG7vzwDeBKwPzEspdSzXvsw+tffnAW2v8lmSJEm93qteoxYRJwLPpJQeiIjja83NQKVbtyagawXt1NqX9unulfZp6rZPXdZbb9DKdFcv19bWWnQJktRreM5dda+1mGAUMCwi/ga8ARhENVgN69ZnKDAdmAWsExF9UkqdtT7Ta32m1fpNjYgWoBWYA0yt9Xtyuc+q25w5C+jqWj4fSi/X1tbK7Nnziy5Dalj+0NXK8pz7kubmptc1uPSqU58ppf1SSm9JKW0PnAvcmVI6AVgUEbvXuh0D3J1SWgJMpBruAI4F7q69Hl/bpvb+xFr/F9sjYg9gUUppykofhSRJUgm93oeyHwVcFRGDgUeAS2vtHweui4izgSnAEbX2c4CxEfE48Hxtf4DLgCtr7Yuphj5JkiQBTZVKw04bbgpMdupT9XLqU1o1bW2tHHTGHUWXoQZx1+hDPOd2023qczPgqbr3W10FSZIkadUY1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTLfV0ioivAIfXNn+ZUvp8ROwLfAdYC7g1pXR2re/2wNXAYGACcEpKqSMiNgFuBDYAEnBUSmlBRKwL3ASMAGYDh6eUZvbYEUqSJDWo1xxRqwWy9wA7ANsDO0XEEcA1wCHAVsDOETGytsuNwGkppS2AJuCkWvsPgR+mlLYEHgbOqbVfAExMKW0FXAVc0hMHJkmS1OjqmfqcAZyRUmpPKS0B/glsAfwrpTQ5pdRBNZx9KCKGA2ullB6q7Tu21t4X2BP4aff22usDqY6oAfwYGFnrL0mS1Ku95tRnSunxpa8j4s1Up0AvoxrglpoBvAnY6BXa1wfm1UJd93a671ObIp0HtAHT6zmA9dYbVE83CYC2ttaiS5CkXsNz7qqr6xo1gIjYBvgl8Dmgg+qo2lJNQBfVEbpKHe3U2pf26a6p23uvac6cBXR1Lf/R0su1tbUye/b8osuQGpY/dLWyPOe+pLm56XUNLtW16jMidgceAL6YUroOmAoM69ZlKNURsFdqnwWsExF9au3DeGnEbFqtHxHRArQCc1b6SCRJkkqmnsUEGwO3A0emlG6pNf+p+lZsXgtfRwJ3p5SeBhbVgh3AMbX2JcBEYFSt/Vjg7trr8bVtau9PrPWXJEnq1eqZ+jwTGAB8JyKWtl0BHA+Mq703npcWChwFXBURg4FHgEtr7R8HrouIs4EpwBG19nOAsRHxOPB8bX9JkqRer6lSadjruzYFJnuNmurlNWrSqmlra+WgM+4ougw1iLtGH+I5t5tu16htBjxV936rqyBJkiStGoOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGWqpegC1PNa1+3PgL79ii4jS21trUWXkJ1FS9qZ//ziosuQJK2AQa2EBvTtx+G3nlp0GWoQt426nPkY1CQpR059SpIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpb89RQu0d7dw26vKiy1CDaO9oL7oESdIrMKiVUL+Wfky68NCiy1CDGHHWOPA+apKUJac+JUmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTBjVJkqRMGdQkSZIyZVCTJEnKlEFNkiQpUwY1SZKkTBnUJEmSMmVQkyRJypRBTZIkKVMGNUmSpEwZ1CRJkjJlUJMkScqUQU2SJClTLUUXoJ7XtaSdEWeNK7oMNYiuJe1FlyBJegUGtRJq7tuPg864o+gy1CDuGn0IsLjoMiRJK+DUpyRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5okSVKmDGqSJEmZMqhJkiRlyqAmSZKUKYOaJElSpgxqkiRJmTKoSZIkZcqgJkmSlKmWogsAiIgjgbOBvsD3Uko/KLgkSZKkwhU+ohYRbwQuBPYAtgdOjoiti61KkiSpeDmMqO0L/Dql9G+AiPgpcBjwtdfYrw9Ac3PT6q2uQW0wZK2iS1AD8f8j1ctzi1aG55aXdPte9FmZ/XIIahsBM7ptzwB2qWO/YQBDhgxcHTU1vDFnv6foEtRA1ltvUNElqEF4btHK8NyyQsOAJ+vtnENQawYq3babgK469vsz8E6qwa5zNdQlSZLUU/pQDWl/XpmdcghqU6kGrqWGAtPr2G8x8LvVUpEkSVLPq3skbakcgtr9wHkR0QYsBA4FTi62JEmSpOIVvuozpTQNOAv4DfA34OaU0v8WW5UkSVLxmiqVymv3kiRJ0hpX+IiaJEmSVsygJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpQpg5pKKyJ+EBE7F12HpHKJiMcj4nMRMbToWlR+3kdNpRURxwHHAhsA1wM3pJRmFluVpEYXEcOpnluOBCYB1wJ3pJSWFFqYSsmgptKLiI2BI4BTgH8AV6eUbi+2KkllEBEfAC4F1gZuAM5PKc0ptiqViUFNpRYRmwFHUw1qU4FbgH2BjpTSsUXWJqkxRcQg4DDgGOCNVEfsbwEOAD6SUnpbgeWpZHJ4KLu0WkTE74ChVE+iB6SUptTarwfFHhTiAAAKIUlEQVSmFVmbpIY2GfgF8NWU0oSljRFxObBfYVWplBxRU2lFxN4ppV8XXYekcomI1pTS/KLrUO9gUFNpRcS1wMv+gaeUPlJAOZJKIiIms+Jzy4gCylHJOfWpMvttt9d9gYOBJ4opRVKJvLvb677AB4D+xZSisnNETb1GRDQBv08pvaPoWiSVS0Q87CICrQ6OqKk32QoYVnQRkhpbROzZbbMJ2AZYq6ByVHIGNZVWRHRRvY6kqdY0G/hicRVJKomvdntdAZ4DjiuoFpWcU5+SJEmZckRNpRQRrcCilNKSiBgF7A78JaV0XcGlSWpgEXEAMAv4O9UnEuwO/AX4QkppVpG1qZwcUVPpRMThwFXAfOBqqs/j+wXVE+ofUkqfKbA8SQ0qIi6k+mSTfsCzwELgOmBvYLOU0iEFlqeSckRNZXQusAUwmOpvvcNTSs9GRH+qv/lK0uvxfmBbqs/1fAZYP6XUAdwZEY8WWplKq7noAqTVoCOl9GxK6V/Av1JKzwKklBYD7cWWJqmBLUkpddaeSvBULaQt1VlUUSo3g5rKqKvb6yWFVSGpbLpe4bW02jj1qTIaHhHXUL0tx9LX1LY3Ka4sSQ1um4iYRPVcslHtNbVt79Go1cKgpjL6bLfXv13uveW3JaleWxRdgHofV32qtCLi3pTS/kXXIalcImJcSunQ5doeSCntU1RNKi9H1FRma0XEximlZ4ouRFLji4ifAduz7LQnVB/MPqWYqlR2BjWVWRvwVETMAl6geh1JJaU0otiyJDWo44E3AJcAp3dr76B6XzWpxxnUVGYHFF2ApPJIKc0D5gGHRMRIYB+qP0d/nVK6s9DiVFrenkOllVJ6murTCE6m+kD2d9XaJOl1i4jPAedRne6cDJwdEWcVWpRKy8UEKq2I+CbwJmAn4O3AHcAjKaUzCi1MUkOLiMeAt6eUXqhtr031WcJbFVuZysgRNZXZ/sAxVB/OPg/YDxhZbEmSSqB5aUirWUT1OjWpx3mNmsps6Z3Dlw4b98e7iUtadQ9ExDhgbG37OODXxZWjMjOoqcxuA24F3hARn6Y6unZzsSVJKoFPA6cCx1Kdmfo1cGWhFam0vEZNpRYR+wP7An2orsz6RcElSZJUN4OaSi0i3goM6d6WUppQUDmSGlhEdPHSpRQvk1LqswbLUS/h1KdKKyJuAXYEpnVrrgB7F1ORpEaWUlpmAV5ENANfAD4DfLmQolR6BjWV2XbAVimlzqILkVQuEbEV1cUEc4GdfFSdVheDmsrsT8DmQCq6EEnlEBFNwBepjqKdlVK6quCSVHIGNZXZA8DjETGd6j2OfNanpNdtuVG0HVNKU4utSL2BQU1l9mWq16P52ChJPeGvtb//CFwfEcu8mVLy+lf1OIOayuw5YGJKyaXNknrC/kUXoN7H23OotCJiDPAW4FdA+9L2lNLXCitKUsOLiHtTSoY2rRE+61NlNgUYDyyhen3a0j+StCrWjoiNiy5CvYMjaiq1iGgD3k51mv+PKaVnCy5JUoOLiH8CWwCzgBdwoZJWI69RU2nVHh91DfAQ1dHjKyPioz5GStIqOqDoAtR7OPWpMrsQ2COldGhK6QPAbsAFBdckqcGllJ4GdgdOBmYD76q1ST3OoKYy65tSmrx0I6U0Cf/NS1pFEfFN4L3AB6nOTJ0QEaOLrUpl5dSnymxKRHwaGFPbPhHvqSZp1e1P9TnCj6SU5kXEfsBjwBnFlqUycnRBZfZRqtOdk4DJtdcnFVqRpDLoqv29dDVe/25tUo9yRE1ltl1KaVT3hoj4IPCzguqRVA63AbcCb6iN2h8D3FxsSSorb8+h0omIUVR/w/0acG63t1qAL6eUNi+kMEmlUVtVvi/QB/i1q8m1ujiipjJqpboiqxXYq1t7B3BWIRVJKpvpwF1LNyJiz5TShALrUUk5oqbSioh9UkoPdNsenFKaV2RNkhpfRNxCdTHBtG7NFR/KrtXBETWV2doRcRFwPvBnoC0izkwpjS22LEkNbjtgq5RSZ9GFqPxc9akyO5fqBb4fBv4X2BT4ZJEFSSqFPwFe66o1wqCmUkspPQocCNyZUloA9C24JEmN7wHg8YiYEhGTImJyREwquiiVk1OfKrNnI+IyYGfg6Nqdw6cUXJOkxvdlYG+8gbbWAIOayuwI4APA91JKC2u/8Z5XbEmSSuA5YGJKydV4Wu0MaiqzBcAg4KKIaAF+AywstiRJJfB/gYci4ldA+9LGlNLXiitJZWVQU5ldDLwZuAZoAk4ARgCfKrIoSQ1vCi9dRtFUZCEqP++jptKKiEeBHVJKXbXtFuDvKaWtiq1MUqOLiDbg7VQHPP6YUnq24JJUUq76VJm1sOwqzxbA+x5JWiW1x0f9jeoo/XHAYxHxvmKrUlk59akyuwn4TUT8uLZ9BD44WdKquxDYI6U0GSAiRgA/A3zep3qcI2oqpYgYAlxF9cHsmwDHA5enlL5eZF2SSqHv0pAGkFKahD9PtZo4oqbSiYgdgPHACSmle4B7IuLrwDcj4tGU0mPFViipwU2JiE8DY2rbJ+I91bSa+BuAyujbwBG1kAZASunLwEeA7xRWlaSy+CiwGzAJmFx7fVKhFam0HFFTGQ1JKf12+caU0r21h7RL0qrYLqU0qntDRHyQ6nVqUo8yqKmM+kZE89LbciwVEc1Av4JqktTgImIU0B/4WkSc2+2tFqqPlTKoqccZ1FRGDwJfqf3p7mzg4TVfjqSSaAV2r/29V7f2DuCsQipS6XnDW5VORLRSXUywMdV7HS0CdgRmAQenlP5dYHmSGlxE7JNSeqDb9uCU0rwia1J5GdRUShHRRPU33h2ALuDhlNLEYquSVAYRcRCwB3A+8GegDTgzpTS2yLpUTgY1SZJWQkT8meotOXYG3gl8AngwpbRToYWplLw9hyRJKyml9ChwIHBnSmkByz6uTuoxBjVJklbOsxFxGdURtXsiYjQwpeCaVFIGNUmSVs4RVK9Ne1dKaSHVG98eUWxJKitvzyFJ0spZAAwCLoqIFuA3wMJiS1JZGdQkSVo5FwNvBq4BmoATgBHAp4osSuVkUJMkaeW8B9hh6dNPIuKXwN+LLUll5TVqkiStnBaWXeXZAnQWVItKzhE1SZJWzk3AbyLix7XtI4CbC6xHJeYNbyVJqlNEDKE6yLETsA+wN/C9lNINhRam0nLqU5KkOkTEDsA/gJ1SSveklD4H3At8MyK2LbY6lZVBTZKk+nwbOCKldM/ShpTSl4GPAN8prCqVmkFNkqT6DEkp/Xb5xpTSvcD6a74c9QYGNUmS6tM3Il72c7PW1q+AetQLGNQkSarPg8BXVtB+NvDwGq5FvYSrPiVJqkNEtALjgY2BvwGLgB2BWcDBKaV/F1ieSsqgJklSnSKiCdgL2AHoAh5OKU0stiqVmUFNkiQpU16jJkmSlCmDmiRJUqYMapIkSZkyqEmSJGXKoCZJkpSp/w82S7DckVD7yAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.index = ['CostomerBuy','CostomerNotBuy']\n",
    "df2.plot(kind='bar',stacked=True, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>regular</td>\n",
       "      <td>0.139740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slim</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wide</td>\n",
       "      <td>0.138767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     style     label\n",
       "0  regular  0.139740\n",
       "1     slim  0.138900\n",
       "2     wide  0.138767"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[[\"style\", \"label\"]].groupby(['style'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RELAX CASUAL</td>\n",
       "      <td>0.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOLF</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FOOTBALL GENERIC</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUNNING</td>\n",
       "      <td>0.138650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAINING</td>\n",
       "      <td>0.137367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDOOR</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category     label\n",
       "3      RELAX CASUAL  0.146700\n",
       "1              GOLF  0.141700\n",
       "0  FOOTBALL GENERIC  0.140100\n",
       "4           RUNNING  0.138650\n",
       "5          TRAINING  0.137367\n",
       "2            INDOOR  0.134800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[[\"category\", \"label\"]].groupby(['category'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productgroup</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARDWARE ACCESSORIES</td>\n",
       "      <td>0.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SHOES</td>\n",
       "      <td>0.139567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SWEATSHIRTS</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SHORTS</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           productgroup     label\n",
       "0  HARDWARE ACCESSORIES  0.140700\n",
       "1                 SHOES  0.139567\n",
       "3           SWEATSHIRTS  0.138900\n",
       "2                SHORTS  0.135100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[[\"productgroup\", \"label\"]].groupby(['productgroup'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article.1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GG8661</td>\n",
       "      <td>0.1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PC6383</td>\n",
       "      <td>0.1426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CB8861</td>\n",
       "      <td>0.1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FG2965</td>\n",
       "      <td>0.1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TX1463</td>\n",
       "      <td>0.1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OC6355</td>\n",
       "      <td>0.1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AC7347</td>\n",
       "      <td>0.1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LI3529</td>\n",
       "      <td>0.1376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP5568</td>\n",
       "      <td>0.1351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>VT7698</td>\n",
       "      <td>0.1348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article.1   label\n",
       "4    GG8661  0.1467\n",
       "7    PC6383  0.1426\n",
       "2    CB8861  0.1417\n",
       "3    FG2965  0.1397\n",
       "8    TX1463  0.1389\n",
       "6    OC6355  0.1381\n",
       "0    AC7347  0.1376\n",
       "5    LI3529  0.1376\n",
       "1    AP5568  0.1351\n",
       "9    VT7698  0.1348"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"article.1\", \"label\"]].groupby(['article.1'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retailweek=df[[\"retailweek\", \"label\"]].groupby(['retailweek'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(df_retailweek['retailweek'], df_retailweek['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_retailweektoSale(num):\n",
    "    Find_retailweektoSale=[]\n",
    "    select_indices = list(np.where(df_retailweek[\"label\"] >num)[0])\n",
    "    for i in range(len(select_indices)):\n",
    "        Find_retailweektoSale.append(df_retailweek['retailweek'].iloc[select_indices[i]])\n",
    "        \n",
    "    return Find_retailweektoSale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-01-22',\n",
       " '2015-08-02',\n",
       " '2016-07-31',\n",
       " '2016-01-24',\n",
       " '2015-01-25',\n",
       " '2015-08-16',\n",
       " '2016-08-07',\n",
       " '2016-07-24',\n",
       " '2015-02-15',\n",
       " '2015-08-09',\n",
       " '2017-02-05',\n",
       " '2016-01-31',\n",
       " '2015-01-18',\n",
       " '2015-02-01',\n",
       " '2016-02-07',\n",
       " '2015-07-26',\n",
       " '2016-07-17',\n",
       " '2015-02-08',\n",
       " '2015-07-19',\n",
       " '2017-01-29',\n",
       " '2016-02-14',\n",
       " '2016-08-14']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Find_retailweektoSale(.3)  #months (1,2,7,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>article</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>retailweek</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>sizes</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   country article  sales  regular_price  current_price     ratio retailweek  \\\n",
       "0  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "1  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "2  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "3  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "4  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "\n",
       "   promo1  promo2  customer_id  ...      style                sizes gender  \\\n",
       "0       0       0       1003.0  ...       slim  xxs,xs,s,m,l,xl,xxl  women   \n",
       "1       0       0       1003.0  ...    regular  xxs,xs,s,m,l,xl,xxl  women   \n",
       "2       0       0       1003.0  ...    regular  xxs,xs,s,m,l,xl,xxl  women   \n",
       "3       0       0       1003.0  ...    regular  xxs,xs,s,m,l,xl,xxl   kids   \n",
       "4       0       0       1003.0  ...    regular  xxs,xs,s,m,l,xl,xxl  women   \n",
       "\n",
       "   rgb_r_main_col rgb_g_main_col rgb_b_main_col rgb_r_sec_col  rgb_g_sec_col  \\\n",
       "0             205            104             57           255            187   \n",
       "1             188            238            104           255            187   \n",
       "2             205            173              0           255            187   \n",
       "3             205            140            149           164            211   \n",
       "4             138             43            226           164            211   \n",
       "\n",
       "   rgb_b_sec_col  label  \n",
       "0            255      0  \n",
       "1            255      0  \n",
       "2            255      0  \n",
       "3            238      0  \n",
       "4            238      0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['retailweek'] = pd.to_datetime(df.retailweek)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country                   object\n",
       "article                   object\n",
       "sales                      int64\n",
       "regular_price            float64\n",
       "current_price            float64\n",
       "ratio                    float64\n",
       "retailweek        datetime64[ns]\n",
       "promo1                     int64\n",
       "promo2                     int64\n",
       "customer_id              float64\n",
       "article.1                 object\n",
       "productgroup              object\n",
       "category                  object\n",
       "cost                     float64\n",
       "style                     object\n",
       "sizes                     object\n",
       "gender                    object\n",
       "rgb_r_main_col             int64\n",
       "rgb_g_main_col             int64\n",
       "rgb_b_main_col             int64\n",
       "rgb_r_sec_col              int64\n",
       "rgb_g_sec_col              int64\n",
       "rgb_b_sec_col              int64\n",
       "label                      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df.retailweek.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>article</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>retailweek</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>...</th>\n",
       "      <th>sizes</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>2016-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>...</td>\n",
       "      <td>xxs,xs,s,m,l,xl,xxl</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   country article  sales  regular_price  current_price     ratio retailweek  \\\n",
       "0  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "1  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "2  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "3  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "4  Germany  YN8639     28           5.95           3.95  0.663866 2016-03-27   \n",
       "\n",
       "   promo1  promo2  customer_id  ...                  sizes gender  \\\n",
       "0       0       0       1003.0  ...    xxs,xs,s,m,l,xl,xxl  women   \n",
       "1       0       0       1003.0  ...    xxs,xs,s,m,l,xl,xxl  women   \n",
       "2       0       0       1003.0  ...    xxs,xs,s,m,l,xl,xxl  women   \n",
       "3       0       0       1003.0  ...    xxs,xs,s,m,l,xl,xxl   kids   \n",
       "4       0       0       1003.0  ...    xxs,xs,s,m,l,xl,xxl  women   \n",
       "\n",
       "  rgb_r_main_col  rgb_g_main_col rgb_b_main_col rgb_r_sec_col rgb_g_sec_col  \\\n",
       "0            205             104             57           255           187   \n",
       "1            188             238            104           255           187   \n",
       "2            205             173              0           255           187   \n",
       "3            205             140            149           164           211   \n",
       "4            138              43            226           164           211   \n",
       "\n",
       "   rgb_b_sec_col  label  month  \n",
       "0            255      0      3  \n",
       "1            255      0      3  \n",
       "2            255      0      3  \n",
       "3            238      0      3  \n",
       "4            238      0      3  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12042d2ddd8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'promo1', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12042de3da0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'promo2', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12042df6400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_r_main_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12043f541d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_g_main_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12043f34a58>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_b_main_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12043095128>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_r_sec_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x120440768d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_g_sec_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x12049b9f518>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'rgb_b_sec_col', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['rgb_main_col']=df.iloc[:,-8:-5].sum(axis=1)\n",
    "#df['rgb_sec_col']=df.iloc[:,-5:-2].sum(axis=1)\n",
    "#df=df.drop(columns=['rgb_r_main_col','rgb_g_main_col','rgb_b_main_col','rgb_r_sec_col','rgb_g_sec_col','rgb_b_sec_col'])\n",
    "df=df.drop(columns=['sizes']) # does not main dependancy factor regarding to the gender\n",
    "df=df.drop(columns=['retailweek']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>article</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article.1</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Germany</td>\n",
       "      <td>YN8639</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Germany</td>\n",
       "      <td>CF3238</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Germany</td>\n",
       "      <td>WR9459</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>France</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>Germany</td>\n",
       "      <td>BF7459</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>OC6355</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>AP5568</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>CB8861</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>LI3529</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>kids</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>GG8661</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>women</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>TX1463</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>PC6383</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>unisex</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>VT7698</td>\n",
       "      <td>...</td>\n",
       "      <td>wide</td>\n",
       "      <td>women</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>FG2965</td>\n",
       "      <td>...</td>\n",
       "      <td>slim</td>\n",
       "      <td>women</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Germany</td>\n",
       "      <td>PW6278</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>AC7347</td>\n",
       "      <td>...</td>\n",
       "      <td>regular</td>\n",
       "      <td>men</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country article  sales  regular_price  current_price     ratio  promo1  \\\n",
       "0      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "1      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "2      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "3      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "4      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "5      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "6      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "7      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "8      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "9      Germany  YN8639     28           5.95           3.95  0.663866       0   \n",
       "10     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "11     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "12     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "13     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "14     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "15     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "16     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "17     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "18     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "19     Germany  CF3238     27          37.95          28.95  0.762846       0   \n",
       "20     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "21     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "22     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "23     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "24     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "25     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "26     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "27     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "28     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "29     Germany  WR9459     59          57.95          44.95  0.775669       0   \n",
       "...        ...     ...    ...            ...            ...       ...     ...   \n",
       "99970   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99971   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99972   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99973   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99974   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99975   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99976   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99977   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99978   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99979   France  AC7347     10         153.95          87.95  0.571289       0   \n",
       "99980  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99981  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99982  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99983  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99984  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99985  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99986  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99987  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99988  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99989  Germany  BF7459    432          35.95          22.95  0.638387       1   \n",
       "99990  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99991  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99992  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99993  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99994  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99995  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99996  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99997  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99998  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "99999  Germany  PW6278    227          57.95          26.95  0.465056       0   \n",
       "\n",
       "       promo2  customer_id article.1  ...      style  gender  rgb_r_main_col  \\\n",
       "0           0       1003.0    OC6355  ...       slim   women             205   \n",
       "1           0       1003.0    AP5568  ...    regular   women             188   \n",
       "2           0       1003.0    CB8861  ...    regular   women             205   \n",
       "3           0       1003.0    LI3529  ...    regular    kids             205   \n",
       "4           0       1003.0    GG8661  ...    regular   women             138   \n",
       "5           0       1003.0    TX1463  ...       wide   women              79   \n",
       "6           0       1003.0    PC6383  ...       wide  unisex             139   \n",
       "7           0       1003.0    VT7698  ...       wide   women             135   \n",
       "8           0       1003.0    FG2965  ...       slim   women             181   \n",
       "9           0       1003.0    AC7347  ...    regular     men             139   \n",
       "10          0       1649.0    OC6355  ...       slim   women             205   \n",
       "11          0       1649.0    AP5568  ...    regular   women             188   \n",
       "12          0       1649.0    CB8861  ...    regular   women             205   \n",
       "13          0       1649.0    LI3529  ...    regular    kids             205   \n",
       "14          0       1649.0    GG8661  ...    regular   women             138   \n",
       "15          0       1649.0    TX1463  ...       wide   women              79   \n",
       "16          0       1649.0    PC6383  ...       wide  unisex             139   \n",
       "17          0       1649.0    VT7698  ...       wide   women             135   \n",
       "18          0       1649.0    FG2965  ...       slim   women             181   \n",
       "19          0       1649.0    AC7347  ...    regular     men             139   \n",
       "20          0        936.0    OC6355  ...       slim   women             205   \n",
       "21          0        936.0    AP5568  ...    regular   women             188   \n",
       "22          0        936.0    CB8861  ...    regular   women             205   \n",
       "23          0        936.0    LI3529  ...    regular    kids             205   \n",
       "24          0        936.0    GG8661  ...    regular   women             138   \n",
       "25          0        936.0    TX1463  ...       wide   women              79   \n",
       "26          0        936.0    PC6383  ...       wide  unisex             139   \n",
       "27          0        936.0    VT7698  ...       wide   women             135   \n",
       "28          0        936.0    FG2965  ...       slim   women             181   \n",
       "29          0        936.0    AC7347  ...    regular     men             139   \n",
       "...       ...          ...       ...  ...        ...     ...             ...   \n",
       "99970       0       3472.0    OC6355  ...       slim   women             205   \n",
       "99971       0       3472.0    AP5568  ...    regular   women             188   \n",
       "99972       0       3472.0    CB8861  ...    regular   women             205   \n",
       "99973       0       3472.0    LI3529  ...    regular    kids             205   \n",
       "99974       0       3472.0    GG8661  ...    regular   women             138   \n",
       "99975       0       3472.0    TX1463  ...       wide   women              79   \n",
       "99976       0       3472.0    PC6383  ...       wide  unisex             139   \n",
       "99977       0       3472.0    VT7698  ...       wide   women             135   \n",
       "99978       0       3472.0    FG2965  ...       slim   women             181   \n",
       "99979       0       3472.0    AC7347  ...    regular     men             139   \n",
       "99980       0        872.0    OC6355  ...       slim   women             205   \n",
       "99981       0        872.0    AP5568  ...    regular   women             188   \n",
       "99982       0        872.0    CB8861  ...    regular   women             205   \n",
       "99983       0        872.0    LI3529  ...    regular    kids             205   \n",
       "99984       0        872.0    GG8661  ...    regular   women             138   \n",
       "99985       0        872.0    TX1463  ...       wide   women              79   \n",
       "99986       0        872.0    PC6383  ...       wide  unisex             139   \n",
       "99987       0        872.0    VT7698  ...       wide   women             135   \n",
       "99988       0        872.0    FG2965  ...       slim   women             181   \n",
       "99989       0        872.0    AC7347  ...    regular     men             139   \n",
       "99990       0       1489.0    OC6355  ...       slim   women             205   \n",
       "99991       0       1489.0    AP5568  ...    regular   women             188   \n",
       "99992       0       1489.0    CB8861  ...    regular   women             205   \n",
       "99993       0       1489.0    LI3529  ...    regular    kids             205   \n",
       "99994       0       1489.0    GG8661  ...    regular   women             138   \n",
       "99995       0       1489.0    TX1463  ...       wide   women              79   \n",
       "99996       0       1489.0    PC6383  ...       wide  unisex             139   \n",
       "99997       0       1489.0    VT7698  ...       wide   women             135   \n",
       "99998       0       1489.0    FG2965  ...       slim   women             181   \n",
       "99999       0       1489.0    AC7347  ...    regular     men             139   \n",
       "\n",
       "      rgb_g_main_col rgb_b_main_col  rgb_r_sec_col  rgb_g_sec_col  \\\n",
       "0                104             57            255            187   \n",
       "1                238            104            255            187   \n",
       "2                173              0            255            187   \n",
       "3                140            149            164            211   \n",
       "4                 43            226            164            211   \n",
       "5                148            205            164            211   \n",
       "6                 26             26            205            155   \n",
       "7                206            250            205            155   \n",
       "8                181            181            205            155   \n",
       "9                137            137            205            155   \n",
       "10               104             57            255            187   \n",
       "11               238            104            255            187   \n",
       "12               173              0            255            187   \n",
       "13               140            149            164            211   \n",
       "14                43            226            164            211   \n",
       "15               148            205            164            211   \n",
       "16                26             26            205            155   \n",
       "17               206            250            205            155   \n",
       "18               181            181            205            155   \n",
       "19               137            137            205            155   \n",
       "20               104             57            255            187   \n",
       "21               238            104            255            187   \n",
       "22               173              0            255            187   \n",
       "23               140            149            164            211   \n",
       "24                43            226            164            211   \n",
       "25               148            205            164            211   \n",
       "26                26             26            205            155   \n",
       "27               206            250            205            155   \n",
       "28               181            181            205            155   \n",
       "29               137            137            205            155   \n",
       "...              ...            ...            ...            ...   \n",
       "99970            104             57            255            187   \n",
       "99971            238            104            255            187   \n",
       "99972            173              0            255            187   \n",
       "99973            140            149            164            211   \n",
       "99974             43            226            164            211   \n",
       "99975            148            205            164            211   \n",
       "99976             26             26            205            155   \n",
       "99977            206            250            205            155   \n",
       "99978            181            181            205            155   \n",
       "99979            137            137            205            155   \n",
       "99980            104             57            255            187   \n",
       "99981            238            104            255            187   \n",
       "99982            173              0            255            187   \n",
       "99983            140            149            164            211   \n",
       "99984             43            226            164            211   \n",
       "99985            148            205            164            211   \n",
       "99986             26             26            205            155   \n",
       "99987            206            250            205            155   \n",
       "99988            181            181            205            155   \n",
       "99989            137            137            205            155   \n",
       "99990            104             57            255            187   \n",
       "99991            238            104            255            187   \n",
       "99992            173              0            255            187   \n",
       "99993            140            149            164            211   \n",
       "99994             43            226            164            211   \n",
       "99995            148            205            164            211   \n",
       "99996             26             26            205            155   \n",
       "99997            206            250            205            155   \n",
       "99998            181            181            205            155   \n",
       "99999            137            137            205            155   \n",
       "\n",
       "       rgb_b_sec_col  label  month  \n",
       "0                255      0      3  \n",
       "1                255      0      3  \n",
       "2                255      0      3  \n",
       "3                238      0      3  \n",
       "4                238      0      3  \n",
       "5                238      1      3  \n",
       "6                155      0      3  \n",
       "7                155      1      3  \n",
       "8                155      0      3  \n",
       "9                155      1      3  \n",
       "10               255      0      1  \n",
       "11               255      0      1  \n",
       "12               255      0      1  \n",
       "13               238      1      1  \n",
       "14               238      0      1  \n",
       "15               238      1      1  \n",
       "16               155      0      1  \n",
       "17               155      1      1  \n",
       "18               155      0      1  \n",
       "19               155      0      1  \n",
       "20               255      0      1  \n",
       "21               255      1      1  \n",
       "22               255      0      1  \n",
       "23               238      1      1  \n",
       "24               238      0      1  \n",
       "25               238      0      1  \n",
       "26               155      0      1  \n",
       "27               155      0      1  \n",
       "28               155      0      1  \n",
       "29               155      0      1  \n",
       "...              ...    ...    ...  \n",
       "99970            255      0      4  \n",
       "99971            255      0      4  \n",
       "99972            255      0      4  \n",
       "99973            238      1      4  \n",
       "99974            238      0      4  \n",
       "99975            238      0      4  \n",
       "99976            155      1      4  \n",
       "99977            155      0      4  \n",
       "99978            155      0      4  \n",
       "99979            155      1      4  \n",
       "99980            255      0     12  \n",
       "99981            255      0     12  \n",
       "99982            255      0     12  \n",
       "99983            238      1     12  \n",
       "99984            238      1     12  \n",
       "99985            238      0     12  \n",
       "99986            155      0     12  \n",
       "99987            155      0     12  \n",
       "99988            155      1     12  \n",
       "99989            155      1     12  \n",
       "99990            255      0      6  \n",
       "99991            255      0      6  \n",
       "99992            255      0      6  \n",
       "99993            238      0      6  \n",
       "99994            238      0      6  \n",
       "99995            238      0      6  \n",
       "99996            155      0      6  \n",
       "99997            155      0      6  \n",
       "99998            155      0      6  \n",
       "99999            155      0      6  \n",
       "\n",
       "[100000 rows x 23 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.FacetGrid(df, col='label')\n",
    "#g.map(plt.hist, 'rgb_main_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.FacetGrid(df, col='label')\n",
    "#g.map(plt.hist, 'rgb_sec_col', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df, col='label')\n",
    "g.map(plt.hist, 'customer_id', bins=50)\n",
    "g.fig.set_figwidth(15)\n",
    "g.fig.set_figheight(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Label Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_df = pd.get_dummies(df_trainData, columns=['country'])\n",
    "df.replace({\"Germany\": 0, \"Austria\": 1,\"France\":2}, inplace=True)\n",
    "df.replace({\"women\": 0, \"kids\": 1,\"unisex\":2,\"men\":3}, inplace=True)\n",
    "df.replace({\"slim\": 0, \"regular\": 1,\"wide\":2}, inplace=True)\n",
    "df.replace({\"RELAX CASUAL\": 0, \"GOLF\": 1,\"FOOTBALL GENERIC\":2,\"RUNNING\":3,\"TRAINING\":4,\"INDOOR\":5}, inplace=True)\n",
    "df.replace({\"HARDWARE ACCESSORIES\": 0, \"SHOES\": 1,\"SWEATSHIRTS\":2,\"SHORTS\":3}, inplace=True)\n",
    "df.replace({\"GG8661\": 0, \"PC6383\": 1,\"CB8861\":2,\"FG2965\":3,\"TX1463\": 4, \"OC6355\": 5,\"AC7347\":6,\"LI3529\":7, \"OC6355\": 5,\"AP5568\":8,\"VT7698\":9}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article.1</th>\n",
       "      <th>productgroup</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>label</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5.95</td>\n",
       "      <td>3.95</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>37.95</td>\n",
       "      <td>28.95</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1649.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>57.95</td>\n",
       "      <td>44.95</td>\n",
       "      <td>0.775669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99970</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99971</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99972</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99973</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99974</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99975</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99976</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99977</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99978</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99979</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>153.95</td>\n",
       "      <td>87.95</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3472.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99980</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99981</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99982</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99983</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99989</th>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>35.95</td>\n",
       "      <td>22.95</td>\n",
       "      <td>0.638387</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99990</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>140</td>\n",
       "      <td>149</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>148</td>\n",
       "      <td>205</td>\n",
       "      <td>164</td>\n",
       "      <td>211</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>57.95</td>\n",
       "      <td>26.95</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>139</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  sales  regular_price  current_price     ratio  promo1  promo2  \\\n",
       "0            0     28           5.95           3.95  0.663866       0       0   \n",
       "1            0     28           5.95           3.95  0.663866       0       0   \n",
       "2            0     28           5.95           3.95  0.663866       0       0   \n",
       "3            0     28           5.95           3.95  0.663866       0       0   \n",
       "4            0     28           5.95           3.95  0.663866       0       0   \n",
       "5            0     28           5.95           3.95  0.663866       0       0   \n",
       "6            0     28           5.95           3.95  0.663866       0       0   \n",
       "7            0     28           5.95           3.95  0.663866       0       0   \n",
       "8            0     28           5.95           3.95  0.663866       0       0   \n",
       "9            0     28           5.95           3.95  0.663866       0       0   \n",
       "10           0     27          37.95          28.95  0.762846       0       0   \n",
       "11           0     27          37.95          28.95  0.762846       0       0   \n",
       "12           0     27          37.95          28.95  0.762846       0       0   \n",
       "13           0     27          37.95          28.95  0.762846       0       0   \n",
       "14           0     27          37.95          28.95  0.762846       0       0   \n",
       "15           0     27          37.95          28.95  0.762846       0       0   \n",
       "16           0     27          37.95          28.95  0.762846       0       0   \n",
       "17           0     27          37.95          28.95  0.762846       0       0   \n",
       "18           0     27          37.95          28.95  0.762846       0       0   \n",
       "19           0     27          37.95          28.95  0.762846       0       0   \n",
       "20           0     59          57.95          44.95  0.775669       0       0   \n",
       "21           0     59          57.95          44.95  0.775669       0       0   \n",
       "22           0     59          57.95          44.95  0.775669       0       0   \n",
       "23           0     59          57.95          44.95  0.775669       0       0   \n",
       "24           0     59          57.95          44.95  0.775669       0       0   \n",
       "25           0     59          57.95          44.95  0.775669       0       0   \n",
       "26           0     59          57.95          44.95  0.775669       0       0   \n",
       "27           0     59          57.95          44.95  0.775669       0       0   \n",
       "28           0     59          57.95          44.95  0.775669       0       0   \n",
       "29           0     59          57.95          44.95  0.775669       0       0   \n",
       "...        ...    ...            ...            ...       ...     ...     ...   \n",
       "99970        2     10         153.95          87.95  0.571289       0       0   \n",
       "99971        2     10         153.95          87.95  0.571289       0       0   \n",
       "99972        2     10         153.95          87.95  0.571289       0       0   \n",
       "99973        2     10         153.95          87.95  0.571289       0       0   \n",
       "99974        2     10         153.95          87.95  0.571289       0       0   \n",
       "99975        2     10         153.95          87.95  0.571289       0       0   \n",
       "99976        2     10         153.95          87.95  0.571289       0       0   \n",
       "99977        2     10         153.95          87.95  0.571289       0       0   \n",
       "99978        2     10         153.95          87.95  0.571289       0       0   \n",
       "99979        2     10         153.95          87.95  0.571289       0       0   \n",
       "99980        0    432          35.95          22.95  0.638387       1       0   \n",
       "99981        0    432          35.95          22.95  0.638387       1       0   \n",
       "99982        0    432          35.95          22.95  0.638387       1       0   \n",
       "99983        0    432          35.95          22.95  0.638387       1       0   \n",
       "99984        0    432          35.95          22.95  0.638387       1       0   \n",
       "99985        0    432          35.95          22.95  0.638387       1       0   \n",
       "99986        0    432          35.95          22.95  0.638387       1       0   \n",
       "99987        0    432          35.95          22.95  0.638387       1       0   \n",
       "99988        0    432          35.95          22.95  0.638387       1       0   \n",
       "99989        0    432          35.95          22.95  0.638387       1       0   \n",
       "99990        0    227          57.95          26.95  0.465056       0       0   \n",
       "99991        0    227          57.95          26.95  0.465056       0       0   \n",
       "99992        0    227          57.95          26.95  0.465056       0       0   \n",
       "99993        0    227          57.95          26.95  0.465056       0       0   \n",
       "99994        0    227          57.95          26.95  0.465056       0       0   \n",
       "99995        0    227          57.95          26.95  0.465056       0       0   \n",
       "99996        0    227          57.95          26.95  0.465056       0       0   \n",
       "99997        0    227          57.95          26.95  0.465056       0       0   \n",
       "99998        0    227          57.95          26.95  0.465056       0       0   \n",
       "99999        0    227          57.95          26.95  0.465056       0       0   \n",
       "\n",
       "       customer_id  article.1  productgroup  ...    style  gender  \\\n",
       "0           1003.0          5             1  ...        0       0   \n",
       "1           1003.0          8             3  ...        1       0   \n",
       "2           1003.0          2             0  ...        1       0   \n",
       "3           1003.0          7             1  ...        1       1   \n",
       "4           1003.0          0             1  ...        1       0   \n",
       "5           1003.0          4             2  ...        2       0   \n",
       "6           1003.0          1             1  ...        2       2   \n",
       "7           1003.0          9             1  ...        2       0   \n",
       "8           1003.0          3             0  ...        0       0   \n",
       "9           1003.0          6             1  ...        1       3   \n",
       "10          1649.0          5             1  ...        0       0   \n",
       "11          1649.0          8             3  ...        1       0   \n",
       "12          1649.0          2             0  ...        1       0   \n",
       "13          1649.0          7             1  ...        1       1   \n",
       "14          1649.0          0             1  ...        1       0   \n",
       "15          1649.0          4             2  ...        2       0   \n",
       "16          1649.0          1             1  ...        2       2   \n",
       "17          1649.0          9             1  ...        2       0   \n",
       "18          1649.0          3             0  ...        0       0   \n",
       "19          1649.0          6             1  ...        1       3   \n",
       "20           936.0          5             1  ...        0       0   \n",
       "21           936.0          8             3  ...        1       0   \n",
       "22           936.0          2             0  ...        1       0   \n",
       "23           936.0          7             1  ...        1       1   \n",
       "24           936.0          0             1  ...        1       0   \n",
       "25           936.0          4             2  ...        2       0   \n",
       "26           936.0          1             1  ...        2       2   \n",
       "27           936.0          9             1  ...        2       0   \n",
       "28           936.0          3             0  ...        0       0   \n",
       "29           936.0          6             1  ...        1       3   \n",
       "...            ...        ...           ...  ...      ...     ...   \n",
       "99970       3472.0          5             1  ...        0       0   \n",
       "99971       3472.0          8             3  ...        1       0   \n",
       "99972       3472.0          2             0  ...        1       0   \n",
       "99973       3472.0          7             1  ...        1       1   \n",
       "99974       3472.0          0             1  ...        1       0   \n",
       "99975       3472.0          4             2  ...        2       0   \n",
       "99976       3472.0          1             1  ...        2       2   \n",
       "99977       3472.0          9             1  ...        2       0   \n",
       "99978       3472.0          3             0  ...        0       0   \n",
       "99979       3472.0          6             1  ...        1       3   \n",
       "99980        872.0          5             1  ...        0       0   \n",
       "99981        872.0          8             3  ...        1       0   \n",
       "99982        872.0          2             0  ...        1       0   \n",
       "99983        872.0          7             1  ...        1       1   \n",
       "99984        872.0          0             1  ...        1       0   \n",
       "99985        872.0          4             2  ...        2       0   \n",
       "99986        872.0          1             1  ...        2       2   \n",
       "99987        872.0          9             1  ...        2       0   \n",
       "99988        872.0          3             0  ...        0       0   \n",
       "99989        872.0          6             1  ...        1       3   \n",
       "99990       1489.0          5             1  ...        0       0   \n",
       "99991       1489.0          8             3  ...        1       0   \n",
       "99992       1489.0          2             0  ...        1       0   \n",
       "99993       1489.0          7             1  ...        1       1   \n",
       "99994       1489.0          0             1  ...        1       0   \n",
       "99995       1489.0          4             2  ...        2       0   \n",
       "99996       1489.0          1             1  ...        2       2   \n",
       "99997       1489.0          9             1  ...        2       0   \n",
       "99998       1489.0          3             0  ...        0       0   \n",
       "99999       1489.0          6             1  ...        1       3   \n",
       "\n",
       "       rgb_r_main_col  rgb_g_main_col  rgb_b_main_col  rgb_r_sec_col  \\\n",
       "0                 205             104              57            255   \n",
       "1                 188             238             104            255   \n",
       "2                 205             173               0            255   \n",
       "3                 205             140             149            164   \n",
       "4                 138              43             226            164   \n",
       "5                  79             148             205            164   \n",
       "6                 139              26              26            205   \n",
       "7                 135             206             250            205   \n",
       "8                 181             181             181            205   \n",
       "9                 139             137             137            205   \n",
       "10                205             104              57            255   \n",
       "11                188             238             104            255   \n",
       "12                205             173               0            255   \n",
       "13                205             140             149            164   \n",
       "14                138              43             226            164   \n",
       "15                 79             148             205            164   \n",
       "16                139              26              26            205   \n",
       "17                135             206             250            205   \n",
       "18                181             181             181            205   \n",
       "19                139             137             137            205   \n",
       "20                205             104              57            255   \n",
       "21                188             238             104            255   \n",
       "22                205             173               0            255   \n",
       "23                205             140             149            164   \n",
       "24                138              43             226            164   \n",
       "25                 79             148             205            164   \n",
       "26                139              26              26            205   \n",
       "27                135             206             250            205   \n",
       "28                181             181             181            205   \n",
       "29                139             137             137            205   \n",
       "...               ...             ...             ...            ...   \n",
       "99970             205             104              57            255   \n",
       "99971             188             238             104            255   \n",
       "99972             205             173               0            255   \n",
       "99973             205             140             149            164   \n",
       "99974             138              43             226            164   \n",
       "99975              79             148             205            164   \n",
       "99976             139              26              26            205   \n",
       "99977             135             206             250            205   \n",
       "99978             181             181             181            205   \n",
       "99979             139             137             137            205   \n",
       "99980             205             104              57            255   \n",
       "99981             188             238             104            255   \n",
       "99982             205             173               0            255   \n",
       "99983             205             140             149            164   \n",
       "99984             138              43             226            164   \n",
       "99985              79             148             205            164   \n",
       "99986             139              26              26            205   \n",
       "99987             135             206             250            205   \n",
       "99988             181             181             181            205   \n",
       "99989             139             137             137            205   \n",
       "99990             205             104              57            255   \n",
       "99991             188             238             104            255   \n",
       "99992             205             173               0            255   \n",
       "99993             205             140             149            164   \n",
       "99994             138              43             226            164   \n",
       "99995              79             148             205            164   \n",
       "99996             139              26              26            205   \n",
       "99997             135             206             250            205   \n",
       "99998             181             181             181            205   \n",
       "99999             139             137             137            205   \n",
       "\n",
       "       rgb_g_sec_col  rgb_b_sec_col  label  month  \n",
       "0                187            255      0      3  \n",
       "1                187            255      0      3  \n",
       "2                187            255      0      3  \n",
       "3                211            238      0      3  \n",
       "4                211            238      0      3  \n",
       "5                211            238      1      3  \n",
       "6                155            155      0      3  \n",
       "7                155            155      1      3  \n",
       "8                155            155      0      3  \n",
       "9                155            155      1      3  \n",
       "10               187            255      0      1  \n",
       "11               187            255      0      1  \n",
       "12               187            255      0      1  \n",
       "13               211            238      1      1  \n",
       "14               211            238      0      1  \n",
       "15               211            238      1      1  \n",
       "16               155            155      0      1  \n",
       "17               155            155      1      1  \n",
       "18               155            155      0      1  \n",
       "19               155            155      0      1  \n",
       "20               187            255      0      1  \n",
       "21               187            255      1      1  \n",
       "22               187            255      0      1  \n",
       "23               211            238      1      1  \n",
       "24               211            238      0      1  \n",
       "25               211            238      0      1  \n",
       "26               155            155      0      1  \n",
       "27               155            155      0      1  \n",
       "28               155            155      0      1  \n",
       "29               155            155      0      1  \n",
       "...              ...            ...    ...    ...  \n",
       "99970            187            255      0      4  \n",
       "99971            187            255      0      4  \n",
       "99972            187            255      0      4  \n",
       "99973            211            238      1      4  \n",
       "99974            211            238      0      4  \n",
       "99975            211            238      0      4  \n",
       "99976            155            155      1      4  \n",
       "99977            155            155      0      4  \n",
       "99978            155            155      0      4  \n",
       "99979            155            155      1      4  \n",
       "99980            187            255      0     12  \n",
       "99981            187            255      0     12  \n",
       "99982            187            255      0     12  \n",
       "99983            211            238      1     12  \n",
       "99984            211            238      1     12  \n",
       "99985            211            238      0     12  \n",
       "99986            155            155      0     12  \n",
       "99987            155            155      0     12  \n",
       "99988            155            155      1     12  \n",
       "99989            155            155      1     12  \n",
       "99990            187            255      0      6  \n",
       "99991            187            255      0      6  \n",
       "99992            187            255      0      6  \n",
       "99993            211            238      0      6  \n",
       "99994            211            238      0      6  \n",
       "99995            211            238      0      6  \n",
       "99996            155            155      0      6  \n",
       "99997            155            155      0      6  \n",
       "99998            155            155      0      6  \n",
       "99999            155            155      0      6  \n",
       "\n",
       "[100000 rows x 22 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainData=df.drop(columns=['label'])\n",
    "df_Label=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss # doctest: +NORMALIZE_WHITESPACE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn  \n",
    "#!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Undersampling for Handling Imbalanced \n",
    "nm = NearMiss()\n",
    "X_res,y_res=nm.fit_sample(df_trainData,df_Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27856, 21), (27856,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.shape,y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13928\n",
       "0    13928\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article.1</th>\n",
       "      <th>productgroup</th>\n",
       "      <th>...</th>\n",
       "      <th>cost</th>\n",
       "      <th>style</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>62.95</td>\n",
       "      <td>42.95</td>\n",
       "      <td>0.682288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24.95</td>\n",
       "      <td>19.95</td>\n",
       "      <td>0.799599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.665971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.95</td>\n",
       "      <td>18.95</td>\n",
       "      <td>0.791232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>13.29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23.95</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.874739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  sales  regular_price  current_price     ratio  promo1  promo2  \\\n",
       "0        0      4          62.95          42.95  0.682288       0       0   \n",
       "1        0      6          24.95          19.95  0.799599       0       0   \n",
       "2        0      8          23.95          15.95  0.665971       0       0   \n",
       "3        0      8          23.95          18.95  0.791232       0       0   \n",
       "4        0      6          23.95          20.95  0.874739       0       0   \n",
       "\n",
       "   customer_id  article.1  productgroup  ...     cost  style  gender  \\\n",
       "0        112.0          8             3  ...     2.29      1       0   \n",
       "1        495.0          2             0  ...     1.70      1       0   \n",
       "2       1012.0          8             3  ...     2.29      1       0   \n",
       "3       1682.0          5             1  ...    13.29      0       0   \n",
       "4        495.0          9             1  ...     5.20      2       0   \n",
       "\n",
       "   rgb_r_main_col  rgb_g_main_col  rgb_b_main_col  rgb_r_sec_col  \\\n",
       "0             188             238             104            255   \n",
       "1             205             173               0            255   \n",
       "2             188             238             104            255   \n",
       "3             205             104              57            255   \n",
       "4             135             206             250            205   \n",
       "\n",
       "   rgb_g_sec_col  rgb_b_sec_col  month  \n",
       "0            187            255      1  \n",
       "1            187            255      7  \n",
       "2            187            255      1  \n",
       "3            187            255      2  \n",
       "4            155            155      8  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res=pd.DataFrame(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'sales', 'regular_price', 'current_price', 'ratio', 'promo1',\n",
       "       'promo2', 'customer_id', 'article.1', 'productgroup', 'category',\n",
       "       'cost', 'style', 'gender', 'rgb_r_main_col', 'rgb_g_main_col',\n",
       "       'rgb_b_main_col', 'rgb_r_sec_col', 'rgb_g_sec_col', 'rgb_b_sec_col',\n",
       "       'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trainData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balance=pd.concat([X_res,y_res], axis=1, join='outer', ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sales</th>\n",
       "      <th>regular_price</th>\n",
       "      <th>current_price</th>\n",
       "      <th>ratio</th>\n",
       "      <th>promo1</th>\n",
       "      <th>promo2</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article.1</th>\n",
       "      <th>productgroup</th>\n",
       "      <th>...</th>\n",
       "      <th>style</th>\n",
       "      <th>gender</th>\n",
       "      <th>rgb_r_main_col</th>\n",
       "      <th>rgb_g_main_col</th>\n",
       "      <th>rgb_b_main_col</th>\n",
       "      <th>rgb_r_sec_col</th>\n",
       "      <th>rgb_g_sec_col</th>\n",
       "      <th>rgb_b_sec_col</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>62.95</td>\n",
       "      <td>42.95</td>\n",
       "      <td>0.682288</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24.95</td>\n",
       "      <td>19.95</td>\n",
       "      <td>0.799599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.95</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.665971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>238</td>\n",
       "      <td>104</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.95</td>\n",
       "      <td>18.95</td>\n",
       "      <td>0.791232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>104</td>\n",
       "      <td>57</td>\n",
       "      <td>255</td>\n",
       "      <td>187</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23.95</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.874739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>206</td>\n",
       "      <td>250</td>\n",
       "      <td>205</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  sales  regular_price  current_price     ratio  promo1  promo2  \\\n",
       "0        0      4          62.95          42.95  0.682288       0       0   \n",
       "1        0      6          24.95          19.95  0.799599       0       0   \n",
       "2        0      8          23.95          15.95  0.665971       0       0   \n",
       "3        0      8          23.95          18.95  0.791232       0       0   \n",
       "4        0      6          23.95          20.95  0.874739       0       0   \n",
       "\n",
       "   customer_id  article.1  productgroup  ...    style  gender  rgb_r_main_col  \\\n",
       "0        112.0          8             3  ...        1       0             188   \n",
       "1        495.0          2             0  ...        1       0             205   \n",
       "2       1012.0          8             3  ...        1       0             188   \n",
       "3       1682.0          5             1  ...        0       0             205   \n",
       "4        495.0          9             1  ...        2       0             135   \n",
       "\n",
       "   rgb_g_main_col  rgb_b_main_col  rgb_r_sec_col  rgb_g_sec_col  \\\n",
       "0             238             104            255            187   \n",
       "1             173               0            255            187   \n",
       "2             238             104            255            187   \n",
       "3             104              57            255            187   \n",
       "4             206             250            205            155   \n",
       "\n",
       "   rgb_b_sec_col  month  label  \n",
       "0            255      1      0  \n",
       "1            255      7      0  \n",
       "2            255      1      0  \n",
       "3            255      2      0  \n",
       "4            155      8      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulise our data After balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.511144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.504065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.499283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.490203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender     label\n",
       "1       1  0.511144\n",
       "2       2  0.504065\n",
       "0       0  0.499283\n",
       "3       3  0.490203"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CostomerBuy = df_balance[df_balance['label']==1]['gender'].value_counts()\n",
    "CostomerNotBuy =df_balance[df_balance['label']==0]['gender'].value_counts()\n",
    "df_balance1 = pd.DataFrame([CostomerBuy,CostomerNotBuy])\n",
    "df_balance[[\"gender\", \"label\"]].groupby(['gender'], as_index=False).mean().sort_values(by='label', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x120504f41d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_balance1.index = ['CostomerBuy','CostomerNotBuy']\n",
    "df_balance1.plot(kind='bar',stacked=True, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1205055b2b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df_balance, col='label')\n",
    "g.map(plt.hist, 'promo1', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x120505eb748>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.FacetGrid(df_balance, col='label')\n",
    "g.map(plt.hist, 'promo2', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(df_trainData,df_Label,test_size=0.2,random_state=42)\n",
    "X_trainb,X_testb,Y_trainb,Y_testb=train_test_split(X_res,y_res,test_size=0.1,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25070, 21), (2786, 21))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainb.shape, X_testb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc_Xb = MinMaxScaler()\n",
    "X_train_scb = sc_Xb.fit_transform(X_trainb)\n",
    "X_test_scb = sc_Xb.transform(X_testb)\n",
    "sc_X = MinMaxScaler()\n",
    "X_train_sc = sc_X.fit_transform(X_train)\n",
    "X_test_sc = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.ensemble as ens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning Imbalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm_clf = SVC(probability=True)\n",
    "svm_clf.fit(X_train_sc,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svm_clf.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17189,     0],\n",
       "       [ 2811,     0]], dtype=int64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, y_pred)   #Biased to majorty class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85945"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46220656645782354"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning balanced DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_clfb = SVC(probability=True)\n",
    "#svm_clfb.fit(X_train_scb,Y_trainb)\n",
    "#svm_parm = {'kernel': ['rbf', 'poly'], \n",
    "#            'C': [1, 5, 50, 100, 500, 1000,1500,2000], \n",
    "#            'degree': [3, 5, 7], \n",
    "#       'gamma':[0.01,0.04,.1,0.2,.3,.4,.6],\n",
    "#           'random_state': [0,1,2,3,4,5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clfs_opt = []\n",
    "#clfs_best_scores = []\n",
    "#clfs_best_param = []\n",
    "#for param in ( svm_parm):\n",
    "#    clf = RandomizedSearchCV(svm_clfb, svm_parm, cv=5)\n",
    "#    clf.fit(X_train_scb,Y_trainb)\n",
    "#    clfs_opt.append(clf.best_estimator_)\n",
    "#    clfs_best_scores.append(clf.best_score_)\n",
    "#    clfs_best_param.append(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(clfs_best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg = np.argmax(clfs_best_scores)\n",
    "#clfs_best_param[arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clfb = SVC(probability=True,kernel='rbf',random_state= 3, gamma=0.1, degree= 7,C= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=7, gamma=0.1, kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=3, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clfb.fit(X_train_scb,Y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preSVMb=svm_clfb.predict(X_test_scb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1016,  366],\n",
       "       [ 326, 1078]], dtype=int64)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_testb, y_preSVMb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516152189519024"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_testb, y_preSVMb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7514921463809006"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_testb, y_preSVMb, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dec_clfb = DecisionTreeClassifier(criterion='entropy')\n",
    "Dec_clfb.fit(X_train_scb,Y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preDecb=Dec_clfb.predict(X_test_scb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7132089016511127"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_testb, y_preDecb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 984,  398],\n",
       "       [ 401, 1003]], dtype=int64)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_testb, y_preDecb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7131955624185475"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_testb, y_preDecb, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      1382\n",
      "           1       0.72      0.71      0.72      1404\n",
      "\n",
      "    accuracy                           0.71      2786\n",
      "   macro avg       0.71      0.71      0.71      2786\n",
      "weighted avg       0.71      0.71      0.71      2786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_testb, y_preDecb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dec_clfb.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6527"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dec_clfb.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(283.871,226.498,'X[4] <= 0.293\\nentropy = 1.0\\nsamples = 25070\\nvalue = [12546, 12524]'),\n",
       " Text(283.803,220.615,'entropy = 0.0\\nsamples = 3889\\nvalue = [3889, 0]'),\n",
       " Text(283.939,220.615,'X[1] <= 0.043\\nentropy = 0.976\\nsamples = 21181\\nvalue = [8657, 12524]'),\n",
       " Text(223.268,214.732,'X[3] <= 0.173\\nentropy = 0.996\\nsamples = 18433\\nvalue = [8519, 9914]'),\n",
       " Text(120.638,208.849,'X[4] <= 0.479\\nentropy = 0.988\\nsamples = 12804\\nvalue = [7211, 5593]'),\n",
       " Text(36.7326,202.966,'X[0] <= 0.75\\nentropy = 0.784\\nsamples = 3407\\nvalue = [2611, 796]'),\n",
       " Text(19.9945,197.083,'X[4] <= 0.342\\nentropy = 0.741\\nsamples = 3165\\nvalue = [2501, 664]'),\n",
       " Text(2.42831,191.2,'X[1] <= 0.03\\nentropy = 0.428\\nsamples = 537\\nvalue = [490, 47]'),\n",
       " Text(1.83529,185.317,'X[2] <= 0.255\\nentropy = 0.357\\nsamples = 458\\nvalue = [427, 31]'),\n",
       " Text(1.05662,179.434,'X[10] <= 0.1\\nentropy = 0.325\\nsamples = 438\\nvalue = [412, 26]'),\n",
       " Text(0.339476,173.551,'X[4] <= 0.321\\nentropy = 0.658\\nsamples = 47\\nvalue = [39, 8]'),\n",
       " Text(0.271581,167.668,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(0.407371,167.668,'X[4] <= 0.327\\nentropy = 0.799\\nsamples = 33\\nvalue = [25, 8]'),\n",
       " Text(0.13579,161.785,'X[1] <= 0.007\\nentropy = 0.991\\nsamples = 9\\nvalue = [4, 5]'),\n",
       " Text(0.0678952,155.902,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(0.203685,155.902,'X[7] <= 0.554\\nentropy = 0.863\\nsamples = 7\\nvalue = [2, 5]'),\n",
       " Text(0.13579,150.018,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(0.271581,150.018,'X[20] <= 0.818\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(0.203685,144.135,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(0.339476,144.135,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.678952,161.785,'X[2] <= 0.129\\nentropy = 0.544\\nsamples = 24\\nvalue = [21, 3]'),\n",
       " Text(0.611056,155.902,'X[7] <= 0.76\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(0.543161,150.018,'X[1] <= 0.007\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(0.475266,144.135,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(0.611056,144.135,'X[4] <= 0.338\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(0.543161,138.252,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.678952,138.252,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.678952,150.018,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(0.746847,155.902,'entropy = 0.0\\nsamples = 18\\nvalue = [18, 0]'),\n",
       " Text(1.77376,173.551,'X[10] <= 0.9\\nentropy = 0.269\\nsamples = 391\\nvalue = [373, 18]'),\n",
       " Text(1.70587,167.668,'X[8] <= 0.167\\nentropy = 0.294\\nsamples = 347\\nvalue = [329, 18]'),\n",
       " Text(1.63797,161.785,'entropy = 0.0\\nsamples = 41\\nvalue = [41, 0]'),\n",
       " Text(1.77376,161.785,'X[4] <= 0.335\\nentropy = 0.323\\nsamples = 306\\nvalue = [288, 18]'),\n",
       " Text(1.20514,155.902,'X[15] <= 0.446\\nentropy = 0.274\\nsamples = 255\\nvalue = [243, 12]'),\n",
       " Text(1.13724,150.018,'entropy = 0.0\\nsamples = 35\\nvalue = [35, 0]'),\n",
       " Text(1.27303,150.018,'X[2] <= 0.209\\nentropy = 0.305\\nsamples = 220\\nvalue = [208, 12]'),\n",
       " Text(1.20514,144.135,'X[1] <= 0.011\\nentropy = 0.323\\nsamples = 204\\nvalue = [192, 12]'),\n",
       " Text(0.814742,138.252,'X[20] <= 0.318\\nentropy = 0.207\\nsamples = 92\\nvalue = [89, 3]'),\n",
       " Text(0.746847,132.369,'X[14] <= 0.837\\nentropy = 0.449\\nsamples = 32\\nvalue = [29, 3]'),\n",
       " Text(0.678952,126.486,'X[4] <= 0.328\\nentropy = 0.672\\nsamples = 17\\nvalue = [14, 3]'),\n",
       " Text(0.611056,120.603,'X[4] <= 0.315\\nentropy = 0.811\\nsamples = 12\\nvalue = [9, 3]'),\n",
       " Text(0.475266,114.72,'X[12] <= 0.25\\nentropy = 0.592\\nsamples = 7\\nvalue = [6, 1]'),\n",
       " Text(0.407371,108.837,'X[0] <= 0.25\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(0.339476,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.475266,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.543161,108.837,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(0.746847,114.72,'X[12] <= 0.25\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(0.678952,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.814742,108.837,'X[3] <= 0.077\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(0.678952,102.954,'X[11] <= 0.43\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(0.611056,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(0.746847,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.950532,102.954,'X[18] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(0.882637,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.01843,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(0.746847,120.603,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(0.814742,126.486,'entropy = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(0.882637,132.369,'entropy = 0.0\\nsamples = 60\\nvalue = [60, 0]'),\n",
       " Text(1.59554,138.252,'X[20] <= 0.5\\nentropy = 0.403\\nsamples = 112\\nvalue = [103, 9]'),\n",
       " Text(1.4258,132.369,'X[1] <= 0.019\\nentropy = 0.247\\nsamples = 73\\nvalue = [70, 3]'),\n",
       " Text(1.3579,126.486,'X[1] <= 0.016\\nentropy = 0.431\\nsamples = 34\\nvalue = [31, 3]'),\n",
       " Text(1.29001,120.603,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(1.4258,120.603,'X[15] <= 0.712\\nentropy = 0.61\\nsamples = 20\\nvalue = [17, 3]'),\n",
       " Text(1.3579,114.72,'X[8] <= 0.722\\nentropy = 0.811\\nsamples = 12\\nvalue = [9, 3]'),\n",
       " Text(1.29001,108.837,'X[7] <= 0.5\\nentropy = 0.881\\nsamples = 10\\nvalue = [7, 3]'),\n",
       " Text(1.22211,102.954,'X[4] <= 0.329\\nentropy = 0.918\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(1.15422,97.0708,'X[18] <= 0.786\\nentropy = 0.954\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(1.01843,91.1877,'X[4] <= 0.313\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(0.950532,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(1.08632,85.3046,'X[14] <= 0.738\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(0.950532,79.4215,'X[1] <= 0.017\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(0.882637,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(1.01843,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.22211,79.4215,'X[1] <= 0.017\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(1.15422,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.29001,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(1.29001,91.1877,'X[3] <= 0.07\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(1.22211,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(1.3579,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.29001,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(1.3579,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(1.4258,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(1.49369,114.72,'entropy = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(1.49369,126.486,'entropy = 0.0\\nsamples = 39\\nvalue = [39, 0]'),\n",
       " Text(1.76527,132.369,'X[13] <= 0.667\\nentropy = 0.619\\nsamples = 39\\nvalue = [33, 6]'),\n",
       " Text(1.69738,126.486,'X[4] <= 0.312\\nentropy = 0.722\\nsamples = 30\\nvalue = [24, 6]'),\n",
       " Text(1.62948,120.603,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(1.76527,120.603,'X[11] <= 0.017\\nentropy = 0.764\\nsamples = 27\\nvalue = [21, 6]'),\n",
       " Text(1.62948,114.72,'X[0] <= 0.25\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(1.56159,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(1.69738,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(1.90106,114.72,'X[7] <= 0.096\\nentropy = 0.667\\nsamples = 23\\nvalue = [19, 4]'),\n",
       " Text(1.83317,108.837,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(1.96896,108.837,'X[4] <= 0.322\\nentropy = 0.787\\nsamples = 17\\nvalue = [13, 4]'),\n",
       " Text(1.83317,102.954,'X[7] <= 0.832\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(1.76527,97.0708,'X[9] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(1.69738,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.83317,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(1.90106,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(2.10475,102.954,'X[8] <= 0.611\\nentropy = 0.619\\nsamples = 13\\nvalue = [11, 2]'),\n",
       " Text(2.03685,97.0708,'X[20] <= 0.818\\nentropy = 0.863\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(1.96896,91.1877,'X[7] <= 0.993\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(1.90106,85.3046,'X[3] <= 0.09\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(1.76527,79.4215,'X[11] <= 0.138\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(1.69738,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(1.83317,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(2.03685,79.4215,'X[18] <= 0.786\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(1.96896,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(2.10475,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(2.03685,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(2.10475,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(2.17265,97.0708,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(1.83317,126.486,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(1.34093,144.135,'entropy = 0.0\\nsamples = 16\\nvalue = [16, 0]'),\n",
       " Text(2.34238,155.902,'X[4] <= 0.34\\nentropy = 0.523\\nsamples = 51\\nvalue = [45, 6]'),\n",
       " Text(2.27449,150.018,'X[1] <= 0.005\\nentropy = 0.684\\nsamples = 33\\nvalue = [27, 6]'),\n",
       " Text(2.20659,144.135,'entropy = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(2.34238,144.135,'X[15] <= 0.866\\nentropy = 0.795\\nsamples = 25\\nvalue = [19, 6]'),\n",
       " Text(2.17265,138.252,'X[4] <= 0.337\\nentropy = 0.702\\nsamples = 21\\nvalue = [17, 4]'),\n",
       " Text(2.03685,132.369,'X[14] <= 0.905\\nentropy = 0.985\\nsamples = 7\\nvalue = [4, 3]'),\n",
       " Text(1.96896,126.486,'X[1] <= 0.01\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(1.90106,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(2.03685,120.603,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(2.10475,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(2.30844,132.369,'X[13] <= 0.667\\nentropy = 0.371\\nsamples = 14\\nvalue = [13, 1]'),\n",
       " Text(2.24054,126.486,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(2.37633,126.486,'X[1] <= 0.016\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(2.30844,120.603,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(2.44423,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(2.51212,138.252,'X[2] <= 0.129\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(2.44423,132.369,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(2.58002,132.369,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(2.41028,150.018,'entropy = 0.0\\nsamples = 18\\nvalue = [18, 0]'),\n",
       " Text(1.84166,167.668,'entropy = 0.0\\nsamples = 44\\nvalue = [44, 0]'),\n",
       " Text(2.61396,179.434,'X[2] <= 0.302\\nentropy = 0.811\\nsamples = 20\\nvalue = [15, 5]'),\n",
       " Text(2.54607,173.551,'X[20] <= 0.682\\nentropy = 0.94\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(2.47817,167.668,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(2.61396,167.668,'X[14] <= 0.643\\nentropy = 0.811\\nsamples = 12\\nvalue = [9, 3]'),\n",
       " Text(2.54607,161.785,'X[2] <= 0.291\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(2.47817,155.902,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(2.61396,155.902,'X[14] <= 0.222\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(2.54607,150.018,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(2.68186,150.018,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(2.68186,161.785,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(2.68186,173.551,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(3.02133,185.317,'X[20] <= 0.136\\nentropy = 0.727\\nsamples = 79\\nvalue = [63, 16]'),\n",
       " Text(2.88554,179.434,'X[12] <= 0.25\\nentropy = 0.722\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(2.81765,173.551,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(2.95344,173.551,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(3.15713,179.434,'X[4] <= 0.34\\nentropy = 0.639\\nsamples = 74\\nvalue = [62, 12]'),\n",
       " Text(3.08923,173.551,'X[14] <= 0.456\\nentropy = 0.581\\nsamples = 72\\nvalue = [62, 10]'),\n",
       " Text(2.88554,167.668,'X[4] <= 0.317\\nentropy = 0.918\\nsamples = 12\\nvalue = [8, 4]'),\n",
       " Text(2.81765,161.785,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(2.95344,161.785,'X[7] <= 0.053\\nentropy = 0.991\\nsamples = 9\\nvalue = [5, 4]'),\n",
       " Text(2.88554,155.902,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(3.02133,155.902,'X[3] <= 0.072\\nentropy = 0.863\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(2.95344,150.018,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(3.08923,150.018,'X[7] <= 0.822\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(3.02133,144.135,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(3.15713,144.135,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.29292,167.668,'X[7] <= 0.099\\nentropy = 0.469\\nsamples = 60\\nvalue = [54, 6]'),\n",
       " Text(3.22502,161.785,'entropy = 0.0\\nsamples = 18\\nvalue = [18, 0]'),\n",
       " Text(3.36081,161.785,'X[8] <= 0.722\\nentropy = 0.592\\nsamples = 42\\nvalue = [36, 6]'),\n",
       " Text(3.29292,155.902,'X[4] <= 0.31\\nentropy = 0.709\\nsamples = 31\\nvalue = [25, 6]'),\n",
       " Text(3.22502,150.018,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(3.36081,150.018,'X[10] <= 0.5\\nentropy = 0.65\\nsamples = 30\\nvalue = [25, 5]'),\n",
       " Text(3.29292,144.135,'X[1] <= 0.042\\nentropy = 0.773\\nsamples = 22\\nvalue = [17, 5]'),\n",
       " Text(3.22502,138.252,'X[20] <= 0.273\\nentropy = 0.831\\nsamples = 19\\nvalue = [14, 5]'),\n",
       " Text(3.15713,132.369,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(3.29292,132.369,'X[0] <= 0.25\\nentropy = 0.896\\nsamples = 16\\nvalue = [11, 5]'),\n",
       " Text(3.22502,126.486,'X[16] <= 0.726\\nentropy = 0.837\\nsamples = 15\\nvalue = [11, 4]'),\n",
       " Text(3.15713,120.603,'X[4] <= 0.32\\nentropy = 0.946\\nsamples = 11\\nvalue = [7, 4]'),\n",
       " Text(3.08923,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(3.22502,114.72,'X[8] <= 0.167\\nentropy = 0.991\\nsamples = 9\\nvalue = [5, 4]'),\n",
       " Text(3.08923,108.837,'X[3] <= 0.095\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(3.02133,102.954,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(3.15713,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(3.36081,108.837,'X[4] <= 0.332\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(3.29292,102.954,'X[3] <= 0.044\\nentropy = 0.971\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(3.15713,97.0708,'X[17] <= 0.725\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(3.08923,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.22502,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(3.42871,97.0708,'X[17] <= 0.725\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(3.36081,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(3.4966,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.42871,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.29292,120.603,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(3.36081,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(3.36081,138.252,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(3.42871,144.135,'entropy = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(3.42871,155.902,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(3.22502,173.551,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(37.5607,191.2,'X[1] <= 0.027\\nentropy = 0.786\\nsamples = 2628\\nvalue = [2011, 617]'),\n",
       " Text(26.1412,185.317,'X[4] <= 0.404\\nentropy = 0.743\\nsamples = 2320\\nvalue = [1831, 489]'),\n",
       " Text(14.9156,179.434,'X[3] <= 0.121\\nentropy = 0.642\\nsamples = 1053\\nvalue = [881, 172]'),\n",
       " Text(13.0335,173.551,'X[7] <= 0.992\\nentropy = 0.614\\nsamples = 950\\nvalue = [806, 144]'),\n",
       " Text(10.536,167.668,'X[10] <= 0.7\\nentropy = 0.606\\nsamples = 944\\nvalue = [804, 140]'),\n",
       " Text(6.92531,161.785,'X[7] <= 0.192\\nentropy = 0.668\\nsamples = 561\\nvalue = [463, 98]'),\n",
       " Text(4.63384,155.902,'X[2] <= 0.085\\nentropy = 0.523\\nsamples = 204\\nvalue = [180, 24]'),\n",
       " Text(4.56595,150.018,'entropy = 0.0\\nsamples = 35\\nvalue = [35, 0]'),\n",
       " Text(4.70174,150.018,'X[13] <= 0.833\\nentropy = 0.589\\nsamples = 169\\nvalue = [145, 24]'),\n",
       " Text(4.2774,144.135,'X[1] <= 0.02\\nentropy = 0.483\\nsamples = 134\\nvalue = [120, 14]'),\n",
       " Text(4.2095,138.252,'X[1] <= 0.002\\nentropy = 0.57\\nsamples = 104\\nvalue = [90, 14]'),\n",
       " Text(3.87002,132.369,'X[3] <= 0.067\\nentropy = 0.94\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(3.80213,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(3.93792,126.486,'X[8] <= 0.556\\nentropy = 0.98\\nsamples = 12\\nvalue = [7, 5]'),\n",
       " Text(3.87002,120.603,'X[7] <= 0.114\\nentropy = 0.946\\nsamples = 11\\nvalue = [7, 4]'),\n",
       " Text(3.80213,114.72,'X[10] <= 0.1\\nentropy = 1.0\\nsamples = 8\\nvalue = [4, 4]'),\n",
       " Text(3.63239,108.837,'X[20] <= 0.545\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(3.5645,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(3.70029,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.97187,108.837,'X[7] <= 0.059\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(3.83608,102.954,'X[17] <= 0.725\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(3.76818,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(3.90397,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.10766,102.954,'X[19] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(4.03976,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(4.17555,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(3.93792,114.72,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(4.00581,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.54898,132.369,'X[7] <= 0.025\\nentropy = 0.469\\nsamples = 90\\nvalue = [81, 9]'),\n",
       " Text(4.37924,126.486,'X[15] <= 0.616\\nentropy = 0.845\\nsamples = 11\\nvalue = [8, 3]'),\n",
       " Text(4.31134,120.603,'X[7] <= 0.014\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(4.17555,114.72,'X[10] <= 0.2\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(4.10766,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.24345,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(4.44713,114.72,'X[8] <= 0.056\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(4.37924,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(4.51503,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(4.44713,120.603,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(4.71871,126.486,'X[7] <= 0.099\\nentropy = 0.388\\nsamples = 79\\nvalue = [73, 6]'),\n",
       " Text(4.65082,120.603,'entropy = 0.0\\nsamples = 25\\nvalue = [25, 0]'),\n",
       " Text(4.78661,120.603,'X[8] <= 0.556\\nentropy = 0.503\\nsamples = 54\\nvalue = [48, 6]'),\n",
       " Text(4.71871,114.72,'X[4] <= 0.39\\nentropy = 0.575\\nsamples = 44\\nvalue = [38, 6]'),\n",
       " Text(4.65082,108.837,'X[7] <= 0.105\\nentropy = 0.629\\nsamples = 38\\nvalue = [32, 6]'),\n",
       " Text(4.48108,102.954,'X[10] <= 0.1\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(4.41319,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.54898,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(4.82056,102.954,'X[1] <= 0.011\\nentropy = 0.581\\nsamples = 36\\nvalue = [31, 5]'),\n",
       " Text(4.68477,97.0708,'X[7] <= 0.166\\nentropy = 0.31\\nsamples = 18\\nvalue = [17, 1]'),\n",
       " Text(4.61687,91.1877,'entropy = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(4.75266,91.1877,'X[18] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(4.68477,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(4.82056,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.95635,97.0708,'X[8] <= 0.056\\nentropy = 0.764\\nsamples = 18\\nvalue = [14, 4]'),\n",
       " Text(4.88845,91.1877,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(5.02424,91.1877,'X[7] <= 0.189\\nentropy = 0.863\\nsamples = 14\\nvalue = [10, 4]'),\n",
       " Text(4.95635,85.3046,'X[8] <= 0.278\\nentropy = 0.918\\nsamples = 12\\nvalue = [8, 4]'),\n",
       " Text(4.82056,79.4215,'X[20] <= 0.136\\nentropy = 0.811\\nsamples = 8\\nvalue = [6, 2]'),\n",
       " Text(4.75266,73.5385,'X[4] <= 0.364\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(4.61687,67.6554,'X[16] <= 0.052\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(4.54898,61.7723,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(4.68477,61.7723,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.88845,67.6554,'X[11] <= 0.376\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(4.82056,61.7723,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(4.95635,61.7723,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(4.88845,73.5385,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(5.09214,79.4215,'X[20] <= 0.136\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(5.02424,73.5385,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(5.16003,73.5385,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(5.09214,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(4.78661,108.837,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(4.8545,114.72,'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0]'),\n",
       " Text(4.34529,138.252,'entropy = 0.0\\nsamples = 30\\nvalue = [30, 0]'),\n",
       " Text(5.12608,144.135,'X[20] <= 0.409\\nentropy = 0.863\\nsamples = 35\\nvalue = [25, 10]'),\n",
       " Text(4.99029,138.252,'X[7] <= 0.064\\nentropy = 0.323\\nsamples = 17\\nvalue = [16, 1]'),\n",
       " Text(4.9224,132.369,'X[7] <= 0.059\\nentropy = 0.65\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(4.8545,126.486,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(4.99029,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(5.05819,132.369,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(5.26188,138.252,'X[1] <= 0.003\\nentropy = 1.0\\nsamples = 18\\nvalue = [9, 9]'),\n",
       " Text(5.19398,132.369,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(5.32977,132.369,'X[7] <= 0.132\\nentropy = 0.971\\nsamples = 15\\nvalue = [6, 9]'),\n",
       " Text(5.19398,126.486,'X[4] <= 0.352\\nentropy = 0.845\\nsamples = 11\\nvalue = [3, 8]'),\n",
       " Text(5.12608,120.603,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(5.26188,120.603,'X[1] <= 0.008\\nentropy = 0.503\\nsamples = 9\\nvalue = [1, 8]'),\n",
       " Text(5.19398,114.72,'X[4] <= 0.378\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(5.12608,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(5.26188,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(5.32977,114.72,'entropy = 0.0\\nsamples = 7\\nvalue = [0, 7]'),\n",
       " Text(5.46556,126.486,'X[7] <= 0.166\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(5.39767,120.603,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(5.53346,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.21677,155.902,'X[3] <= 0.08\\nentropy = 0.736\\nsamples = 357\\nvalue = [283, 74]'),\n",
       " Text(7.48756,150.018,'X[18] <= 0.286\\nentropy = 0.816\\nsamples = 241\\nvalue = [180, 61]'),\n",
       " Text(5.90688,144.135,'X[7] <= 0.242\\nentropy = 0.687\\nsamples = 120\\nvalue = [98, 22]'),\n",
       " Text(5.83898,138.252,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(5.97477,138.252,'X[7] <= 0.249\\nentropy = 0.737\\nsamples = 106\\nvalue = [84, 22]'),\n",
       " Text(5.66925,132.369,'X[16] <= 0.636\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(5.60135,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(5.73714,126.486,'X[20] <= 0.318\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(5.66925,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(5.80504,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(6.2803,132.369,'X[7] <= 0.253\\nentropy = 0.694\\nsamples = 102\\nvalue = [83, 19]'),\n",
       " Text(6.21241,126.486,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(6.3482,126.486,'X[7] <= 0.32\\nentropy = 0.73\\nsamples = 93\\nvalue = [74, 19]'),\n",
       " Text(5.94083,120.603,'X[14] <= 0.643\\nentropy = 0.989\\nsamples = 16\\nvalue = [9, 7]'),\n",
       " Text(5.77109,114.72,'X[20] <= 0.682\\nentropy = 0.845\\nsamples = 11\\nvalue = [8, 3]'),\n",
       " Text(5.6353,108.837,'X[7] <= 0.257\\nentropy = 0.544\\nsamples = 8\\nvalue = [7, 1]'),\n",
       " Text(5.5674,102.954,'X[8] <= 0.389\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(5.49951,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(5.6353,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(5.70319,102.954,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(5.90688,108.837,'X[20] <= 0.864\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(5.83898,102.954,'X[8] <= 0.389\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(5.77109,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(5.90688,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(5.97477,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(6.11056,114.72,'X[7] <= 0.267\\nentropy = 0.722\\nsamples = 5\\nvalue = [1, 4]'),\n",
       " Text(6.04267,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(6.17846,108.837,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(6.75557,120.603,'X[20] <= 0.818\\nentropy = 0.624\\nsamples = 77\\nvalue = [65, 12]'),\n",
       " Text(6.68767,114.72,'X[3] <= 0.064\\nentropy = 0.667\\nsamples = 69\\nvalue = [57, 12]'),\n",
       " Text(6.31425,108.837,'X[1] <= 0.021\\nentropy = 0.79\\nsamples = 38\\nvalue = [29, 9]'),\n",
       " Text(6.17846,102.954,'X[20] <= 0.136\\nentropy = 0.672\\nsamples = 34\\nvalue = [28, 6]'),\n",
       " Text(6.11056,97.0708,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(6.24635,97.0708,'X[4] <= 0.384\\nentropy = 0.795\\nsamples = 25\\nvalue = [19, 6]'),\n",
       " Text(5.97477,91.1877,'X[20] <= 0.682\\nentropy = 0.503\\nsamples = 18\\nvalue = [16, 2]'),\n",
       " Text(5.90688,85.3046,'entropy = 0.0\\nsamples = 13\\nvalue = [13, 0]'),\n",
       " Text(6.04267,85.3046,'X[15] <= 0.262\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(5.97477,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(6.11056,79.4215,'X[10] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(6.04267,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(6.17846,73.5385,'X[0] <= 0.25\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(6.11056,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(6.24635,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(6.51794,91.1877,'X[11] <= 0.667\\nentropy = 0.985\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(6.45004,85.3046,'X[2] <= 0.034\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(6.38215,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(6.51794,79.4215,'X[15] <= 0.627\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(6.45004,73.5385,'X[1] <= 0.005\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(6.38215,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(6.51794,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(6.58583,73.5385,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(6.58583,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(6.45004,102.954,'X[12] <= 0.75\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(6.38215,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(6.51794,97.0708,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(7.0611,108.837,'X[1] <= 0.011\\nentropy = 0.459\\nsamples = 31\\nvalue = [28, 3]'),\n",
       " Text(6.9932,102.954,'X[12] <= 0.75\\nentropy = 0.672\\nsamples = 17\\nvalue = [14, 3]'),\n",
       " Text(6.92531,97.0708,'X[2] <= 0.137\\nentropy = 0.779\\nsamples = 13\\nvalue = [10, 3]'),\n",
       " Text(6.85741,91.1877,'X[20] <= 0.045\\nentropy = 0.845\\nsamples = 11\\nvalue = [8, 3]'),\n",
       " Text(6.78952,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(6.92531,85.3046,'X[15] <= 0.627\\nentropy = 0.918\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(6.78952,79.4215,'X[7] <= 0.983\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(6.72162,73.5385,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(6.85741,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(7.0611,79.4215,'X[7] <= 0.916\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(6.9932,73.5385,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(7.12899,73.5385,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(6.9932,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(7.0611,97.0708,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(7.12899,102.954,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(6.82346,114.72,'entropy = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(9.06825,144.135,'X[2] <= 0.126\\nentropy = 0.907\\nsamples = 121\\nvalue = [82, 39]'),\n",
       " Text(8.25775,138.252,'X[20] <= 0.409\\nentropy = 0.853\\nsamples = 97\\nvalue = [70, 27]'),\n",
       " Text(7.7061,132.369,'X[7] <= 0.947\\nentropy = 0.959\\nsamples = 42\\nvalue = [26, 16]'),\n",
       " Text(7.63821,126.486,'X[4] <= 0.388\\nentropy = 0.934\\nsamples = 40\\nvalue = [26, 14]'),\n",
       " Text(7.33268,120.603,'X[17] <= 0.5\\nentropy = 0.755\\nsamples = 23\\nvalue = [18, 5]'),\n",
       " Text(7.26478,114.72,'X[1] <= 0.005\\nentropy = 0.896\\nsamples = 16\\nvalue = [11, 5]'),\n",
       " Text(7.19689,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(7.33268,108.837,'X[7] <= 0.213\\nentropy = 0.75\\nsamples = 14\\nvalue = [11, 3]'),\n",
       " Text(7.26478,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(7.40057,102.954,'X[15] <= 0.309\\nentropy = 0.619\\nsamples = 13\\nvalue = [11, 2]'),\n",
       " Text(7.33268,97.0708,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(7.46847,97.0708,'X[1] <= 0.023\\nentropy = 0.811\\nsamples = 8\\nvalue = [6, 2]'),\n",
       " Text(7.40057,91.1877,'X[2] <= 0.021\\nentropy = 0.592\\nsamples = 7\\nvalue = [6, 1]'),\n",
       " Text(7.33268,85.3046,'X[1] <= 0.009\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(7.26478,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(7.40057,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(7.46847,85.3046,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(7.53636,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(7.40057,114.72,'entropy = 0.0\\nsamples = 7\\nvalue = [7, 0]'),\n",
       " Text(7.94373,120.603,'X[20] <= 0.227\\nentropy = 0.998\\nsamples = 17\\nvalue = [8, 9]'),\n",
       " Text(7.80794,114.72,'X[15] <= 0.616\\nentropy = 0.811\\nsamples = 8\\nvalue = [2, 6]'),\n",
       " Text(7.74005,108.837,'X[15] <= 0.309\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(7.67215,102.954,'X[20] <= 0.045\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(7.60426,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(7.74005,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(7.80794,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(7.87584,108.837,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(8.07952,114.72,'X[0] <= 0.25\\nentropy = 0.918\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(8.01163,108.837,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(8.14742,108.837,'X[20] <= 0.318\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(8.01163,102.954,'X[18] <= 0.786\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(7.94373,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(8.07952,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(8.28321,102.954,'X[18] <= 0.786\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(8.21531,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(8.3511,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(7.774,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(8.8094,132.369,'X[7] <= 0.199\\nentropy = 0.722\\nsamples = 55\\nvalue = [44, 11]'),\n",
       " Text(8.7415,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(8.87729,126.486,'X[7] <= 0.263\\nentropy = 0.691\\nsamples = 54\\nvalue = [44, 10]'),\n",
       " Text(8.8094,120.603,'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0]'),\n",
       " Text(8.94519,120.603,'X[7] <= 0.761\\nentropy = 0.773\\nsamples = 44\\nvalue = [34, 10]'),\n",
       " Text(8.62269,114.72,'X[8] <= 0.111\\nentropy = 0.964\\nsamples = 18\\nvalue = [11, 7]'),\n",
       " Text(8.4869,108.837,'X[20] <= 0.818\\nentropy = 0.863\\nsamples = 7\\nvalue = [2, 5]'),\n",
       " Text(8.419,102.954,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(8.55479,102.954,'X[4] <= 0.388\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(8.4869,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(8.62269,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(8.75848,108.837,'X[20] <= 0.818\\nentropy = 0.684\\nsamples = 11\\nvalue = [9, 2]'),\n",
       " Text(8.69058,102.954,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(8.82637,102.954,'X[0] <= 0.25\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(8.75848,97.0708,'X[11] <= 0.338\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(8.69058,91.1877,'X[2] <= 0.052\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(8.62269,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(8.75848,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(8.82637,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(8.89427,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.26769,114.72,'X[4] <= 0.374\\nentropy = 0.516\\nsamples = 26\\nvalue = [23, 3]'),\n",
       " Text(9.19979,108.837,'X[2] <= 0.103\\nentropy = 0.75\\nsamples = 14\\nvalue = [11, 3]'),\n",
       " Text(9.1319,102.954,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(9.26769,102.954,'X[20] <= 0.545\\nentropy = 0.881\\nsamples = 10\\nvalue = [7, 3]'),\n",
       " Text(9.03006,97.0708,'X[8] <= 0.5\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(8.96216,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(9.09795,91.1877,'X[7] <= 0.916\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(9.03006,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(9.16585,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.50532,97.0708,'X[10] <= 0.4\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(9.43743,91.1877,'X[17] <= 0.5\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(9.30164,85.3046,'X[7] <= 0.806\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(9.23374,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(9.36953,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.57322,85.3046,'X[7] <= 0.806\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(9.50532,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.64111,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(9.57322,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(9.33558,108.837,'entropy = 0.0\\nsamples = 12\\nvalue = [12, 0]'),\n",
       " Text(9.87875,138.252,'X[20] <= 0.318\\nentropy = 1.0\\nsamples = 24\\nvalue = [12, 12]'),\n",
       " Text(9.74296,132.369,'X[1] <= 0.007\\nentropy = 0.722\\nsamples = 10\\nvalue = [8, 2]'),\n",
       " Text(9.67506,126.486,'X[17] <= 0.5\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(9.60717,120.603,'X[14] <= 0.734\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(9.53927,114.72,'X[20] <= 0.045\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(9.47138,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(9.60717,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.67506,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(9.74296,120.603,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(9.81085,126.486,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(10.0145,132.369,'X[11] <= 0.667\\nentropy = 0.863\\nsamples = 14\\nvalue = [4, 10]'),\n",
       " Text(9.94664,126.486,'entropy = 0.0\\nsamples = 8\\nvalue = [0, 8]'),\n",
       " Text(10.0824,126.486,'X[20] <= 0.682\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(10.0145,120.603,'X[7] <= 0.282\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(9.94664,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(10.0824,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(10.1503,120.603,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(10.946,150.018,'X[15] <= 0.04\\nentropy = 0.506\\nsamples = 116\\nvalue = [103, 13]'),\n",
       " Text(10.8781,144.135,'entropy = 0.0\\nsamples = 16\\nvalue = [16, 0]'),\n",
       " Text(11.0139,144.135,'X[20] <= 0.045\\nentropy = 0.557\\nsamples = 100\\nvalue = [87, 13]'),\n",
       " Text(10.946,138.252,'entropy = 0.0\\nsamples = 8\\nvalue = [8, 0]'),\n",
       " Text(11.0818,138.252,'X[4] <= 0.351\\nentropy = 0.588\\nsamples = 92\\nvalue = [79, 13]'),\n",
       " Text(11.0139,132.369,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(11.1497,132.369,'X[19] <= 0.415\\nentropy = 0.608\\nsamples = 87\\nvalue = [74, 13]'),\n",
       " Text(10.4219,126.486,'X[3] <= 0.085\\nentropy = 0.431\\nsamples = 34\\nvalue = [31, 3]'),\n",
       " Text(10.354,120.603,'X[20] <= 0.136\\nentropy = 0.629\\nsamples = 19\\nvalue = [16, 3]'),\n",
       " Text(10.2182,114.72,'X[12] <= 0.25\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(10.1503,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(10.2861,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(10.4898,114.72,'X[20] <= 0.591\\nentropy = 0.523\\nsamples = 17\\nvalue = [15, 2]'),\n",
       " Text(10.4219,108.837,'entropy = 0.0\\nsamples = 10\\nvalue = [10, 0]'),\n",
       " Text(10.5577,108.837,'X[20] <= 0.864\\nentropy = 0.863\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(10.4898,102.954,'X[1] <= 0.003\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(10.4219,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(10.5577,97.0708,'X[2] <= 0.144\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(10.4219,91.1877,'X[8] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(10.354,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(10.4898,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(10.6935,91.1877,'X[9] <= 0.167\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(10.6256,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(10.7614,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(10.6256,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(10.4898,120.603,'entropy = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(11.8774,126.486,'X[20] <= 0.5\\nentropy = 0.699\\nsamples = 53\\nvalue = [43, 10]'),\n",
       " Text(11.3979,120.603,'X[0] <= 0.25\\nentropy = 0.855\\nsamples = 25\\nvalue = [18, 7]'),\n",
       " Text(11.016,114.72,'X[10] <= 0.1\\nentropy = 0.94\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(10.8293,108.837,'X[7] <= 0.202\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(10.7614,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(10.8972,102.954,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(11.2027,108.837,'X[7] <= 0.202\\nentropy = 0.971\\nsamples = 10\\nvalue = [6, 4]'),\n",
       " Text(11.033,102.954,'X[10] <= 0.4\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(10.9651,97.0708,'X[7] <= 0.197\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(10.8972,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(11.033,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(11.1009,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(11.3724,102.954,'X[9] <= 0.167\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(11.2366,97.0708,'X[2] <= 0.173\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(11.1688,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(11.3045,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(11.5082,97.0708,'X[4] <= 0.378\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(11.4403,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(11.5761,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(11.7798,114.72,'X[7] <= 0.728\\nentropy = 0.684\\nsamples = 11\\nvalue = [9, 2]'),\n",
       " Text(11.7119,108.837,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(11.8477,108.837,'X[1] <= 0.011\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(11.7798,102.954,'X[9] <= 0.167\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(11.7119,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(11.8477,97.0708,'X[11] <= 0.667\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(11.7119,91.1877,'X[1] <= 0.005\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(11.644,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(11.7798,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(11.9835,91.1877,'X[4] <= 0.38\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(11.9156,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.0514,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(11.9156,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.3569,120.603,'X[4] <= 0.387\\nentropy = 0.491\\nsamples = 28\\nvalue = [25, 3]'),\n",
       " Text(12.289,114.72,'X[1] <= 0.017\\nentropy = 0.629\\nsamples = 19\\nvalue = [16, 3]'),\n",
       " Text(12.1193,108.837,'X[4] <= 0.382\\nentropy = 0.371\\nsamples = 14\\nvalue = [13, 1]'),\n",
       " Text(12.0514,102.954,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(12.1872,102.954,'X[11] <= 0.338\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(12.1193,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.2551,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(12.4588,108.837,'X[9] <= 0.167\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(12.3909,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.5267,102.954,'X[14] <= 0.734\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(12.3909,97.0708,'X[5] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(12.323,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.4588,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.6624,97.0708,'X[20] <= 0.773\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(12.5946,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.7303,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.4248,114.72,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(14.1466,161.785,'X[3] <= 0.054\\nentropy = 0.499\\nsamples = 383\\nvalue = [341, 42]'),\n",
       " Text(13.0337,155.902,'X[20] <= 0.409\\nentropy = 0.678\\nsamples = 106\\nvalue = [87, 19]'),\n",
       " Text(12.7643,150.018,'X[20] <= 0.136\\nentropy = 0.414\\nsamples = 48\\nvalue = [44, 4]'),\n",
       " Text(12.6964,144.135,'X[1] <= 0.011\\nentropy = 0.605\\nsamples = 27\\nvalue = [23, 4]'),\n",
       " Text(12.6285,138.252,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(12.7643,138.252,'X[19] <= 0.915\\nentropy = 0.89\\nsamples = 13\\nvalue = [9, 4]'),\n",
       " Text(12.6964,132.369,'X[4] <= 0.366\\nentropy = 1.0\\nsamples = 8\\nvalue = [4, 4]'),\n",
       " Text(12.6285,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.7643,126.486,'X[17] <= 0.225\\nentropy = 0.985\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(12.6285,120.603,'X[4] <= 0.388\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(12.5606,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(12.6964,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.9001,120.603,'X[1] <= 0.021\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(12.8322,114.72,'X[7] <= 0.7\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(12.7643,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(12.9001,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.968,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(12.8322,132.369,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(12.8322,144.135,'entropy = 0.0\\nsamples = 21\\nvalue = [21, 0]'),\n",
       " Text(13.3032,150.018,'X[2] <= 0.031\\nentropy = 0.825\\nsamples = 58\\nvalue = [43, 15]'),\n",
       " Text(13.1038,144.135,'X[15] <= 0.472\\nentropy = 0.323\\nsamples = 17\\nvalue = [16, 1]'),\n",
       " Text(13.0359,138.252,'X[7] <= 0.14\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(12.968,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(13.1038,132.369,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(13.1717,138.252,'entropy = 0.0\\nsamples = 13\\nvalue = [13, 0]'),\n",
       " Text(13.5027,144.135,'X[7] <= 0.063\\nentropy = 0.926\\nsamples = 41\\nvalue = [27, 14]'),\n",
       " Text(13.3075,138.252,'X[16] <= 0.322\\nentropy = 0.469\\nsamples = 10\\nvalue = [9, 1]'),\n",
       " Text(13.2396,132.369,'X[20] <= 0.955\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(13.1717,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(13.3075,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(13.3753,132.369,'entropy = 0.0\\nsamples = 7\\nvalue = [7, 0]'),\n",
       " Text(13.6978,138.252,'X[1] <= 0.004\\nentropy = 0.981\\nsamples = 31\\nvalue = [18, 13]'),\n",
       " Text(13.5111,132.369,'X[18] <= 0.786\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(13.4432,126.486,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(13.579,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(13.8846,132.369,'X[20] <= 0.5\\nentropy = 0.951\\nsamples = 27\\nvalue = [17, 10]'),\n",
       " Text(13.7148,126.486,'X[12] <= 0.25\\nentropy = 0.918\\nsamples = 6\\nvalue = [2, 4]'),\n",
       " Text(13.6469,120.603,'X[4] <= 0.352\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(13.579,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(13.7148,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(13.7827,120.603,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(14.0543,126.486,'X[1] <= 0.017\\nentropy = 0.863\\nsamples = 21\\nvalue = [15, 6]'),\n",
       " Text(13.9185,120.603,'X[16] <= 0.322\\nentropy = 0.544\\nsamples = 8\\nvalue = [7, 1]'),\n",
       " Text(13.8506,114.72,'X[3] <= 0.049\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(13.7827,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(13.9185,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(13.9864,114.72,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(14.1901,120.603,'X[17] <= 0.225\\nentropy = 0.961\\nsamples = 13\\nvalue = [8, 5]'),\n",
       " Text(14.1222,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.258,114.72,'X[3] <= 0.046\\nentropy = 0.918\\nsamples = 12\\nvalue = [8, 4]'),\n",
       " Text(14.1222,108.837,'X[9] <= 0.667\\nentropy = 0.65\\nsamples = 6\\nvalue = [5, 1]'),\n",
       " Text(14.0543,102.954,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(14.1901,102.954,'X[3] <= 0.036\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(14.1222,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.258,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.3938,108.837,'X[14] <= 0.655\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(14.3259,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.4617,102.954,'X[2] <= 0.09\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(14.3938,97.0708,'X[1] <= 0.022\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(14.258,91.1877,'X[9] <= 0.667\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(14.1901,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.3259,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.5296,91.1877,'X[9] <= 0.667\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(14.4617,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.5975,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.5296,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(15.2594,155.902,'X[20] <= 0.136\\nentropy = 0.413\\nsamples = 277\\nvalue = [254, 23]'),\n",
       " Text(14.6484,150.018,'X[7] <= 0.145\\nentropy = 0.672\\nsamples = 51\\nvalue = [42, 9]'),\n",
       " Text(14.5805,144.135,'entropy = 0.0\\nsamples = 20\\nvalue = [20, 0]'),\n",
       " Text(14.7163,144.135,'X[3] <= 0.064\\nentropy = 0.869\\nsamples = 31\\nvalue = [22, 9]'),\n",
       " Text(14.6484,138.252,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(14.7842,138.252,'X[1] <= 0.004\\nentropy = 0.906\\nsamples = 28\\nvalue = [19, 9]'),\n",
       " Text(14.5975,132.369,'X[14] <= 0.933\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(14.5296,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(14.6654,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.9709,132.369,'X[11] <= 0.284\\nentropy = 0.855\\nsamples = 25\\nvalue = [18, 7]'),\n",
       " Text(14.8011,126.486,'X[3] <= 0.088\\nentropy = 0.65\\nsamples = 12\\nvalue = [10, 2]'),\n",
       " Text(14.7333,120.603,'X[1] <= 0.007\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(14.6654,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(14.8011,114.72,'X[20] <= 0.045\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(14.6654,108.837,'X[17] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(14.5975,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.7333,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(14.9369,108.837,'X[17] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(14.869,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(15.0048,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(14.869,120.603,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(15.1406,126.486,'X[1] <= 0.008\\nentropy = 0.961\\nsamples = 13\\nvalue = [8, 5]'),\n",
       " Text(15.0727,120.603,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(15.2085,120.603,'X[20] <= 0.045\\nentropy = 0.722\\nsamples = 10\\nvalue = [8, 2]'),\n",
       " Text(15.1406,114.72,'X[4] <= 0.357\\nentropy = 0.863\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(15.0727,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(15.2085,108.837,'X[2] <= 0.129\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(15.1406,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(15.2764,102.954,'X[18] <= 0.286\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(15.1406,97.0708,'X[2] <= 0.165\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(15.0727,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(15.2085,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(15.4122,97.0708,'X[3] <= 0.093\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(15.3443,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(15.4801,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(15.2764,114.72,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(15.8705,150.018,'X[20] <= 0.409\\nentropy = 0.335\\nsamples = 226\\nvalue = [212, 14]'),\n",
       " Text(15.548,144.135,'X[7] <= 0.834\\nentropy = 0.1\\nsamples = 77\\nvalue = [76, 1]'),\n",
       " Text(15.4801,138.252,'entropy = 0.0\\nsamples = 58\\nvalue = [58, 0]'),\n",
       " Text(15.6159,138.252,'X[7] <= 0.848\\nentropy = 0.297\\nsamples = 19\\nvalue = [18, 1]'),\n",
       " Text(15.548,132.369,'X[14] <= 0.222\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(15.4801,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(15.6159,126.486,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(15.6838,132.369,'entropy = 0.0\\nsamples = 15\\nvalue = [15, 0]'),\n",
       " Text(16.193,144.135,'X[2] <= 0.175\\nentropy = 0.427\\nsamples = 149\\nvalue = [136, 13]'),\n",
       " Text(15.8875,138.252,'X[8] <= 0.944\\nentropy = 0.321\\nsamples = 120\\nvalue = [113, 7]'),\n",
       " Text(15.8196,132.369,'X[2] <= 0.147\\nentropy = 0.388\\nsamples = 92\\nvalue = [85, 7]'),\n",
       " Text(15.7517,126.486,'X[2] <= 0.131\\nentropy = 0.46\\nsamples = 72\\nvalue = [65, 7]'),\n",
       " Text(15.6159,120.603,'X[20] <= 0.5\\nentropy = 0.262\\nsamples = 45\\nvalue = [43, 2]'),\n",
       " Text(15.548,114.72,'X[12] <= 0.75\\nentropy = 0.544\\nsamples = 16\\nvalue = [14, 2]'),\n",
       " Text(15.4801,108.837,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(15.6159,108.837,'X[1] <= 0.017\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(15.548,102.954,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(15.6838,102.954,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(15.6838,114.72,'entropy = 0.0\\nsamples = 29\\nvalue = [29, 0]'),\n",
       " Text(15.8875,120.603,'X[5] <= 0.5\\nentropy = 0.691\\nsamples = 27\\nvalue = [22, 5]'),\n",
       " Text(15.8196,114.72,'X[20] <= 0.5\\nentropy = 0.792\\nsamples = 21\\nvalue = [16, 5]'),\n",
       " Text(15.7517,108.837,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(15.8875,108.837,'X[1] <= 0.026\\nentropy = 0.874\\nsamples = 17\\nvalue = [12, 5]'),\n",
       " Text(15.8196,102.954,'X[11] <= 0.163\\nentropy = 0.94\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(15.6838,97.0708,'X[20] <= 0.682\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(15.6159,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(15.7517,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(15.9554,97.0708,'X[20] <= 0.682\\nentropy = 0.845\\nsamples = 11\\nvalue = [8, 3]'),\n",
       " Text(15.8875,91.1877,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(16.0233,91.1877,'X[19] <= 0.915\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(15.8875,85.3046,'X[2] <= 0.142\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(15.8196,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(15.9554,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.159,85.3046,'X[0] <= 0.25\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(16.0912,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(16.2269,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(15.9554,102.954,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(15.9554,114.72,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(15.8875,126.486,'entropy = 0.0\\nsamples = 20\\nvalue = [20, 0]'),\n",
       " Text(15.9554,132.369,'entropy = 0.0\\nsamples = 28\\nvalue = [28, 0]'),\n",
       " Text(16.4985,138.252,'X[7] <= 0.804\\nentropy = 0.736\\nsamples = 29\\nvalue = [23, 6]'),\n",
       " Text(16.4306,132.369,'X[9] <= 0.833\\nentropy = 0.605\\nsamples = 27\\nvalue = [23, 4]'),\n",
       " Text(16.3627,126.486,'X[7] <= 0.139\\nentropy = 0.764\\nsamples = 18\\nvalue = [14, 4]'),\n",
       " Text(16.159,120.603,'X[8] <= 0.778\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(16.0912,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.2269,114.72,'X[1] <= 0.012\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(16.159,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(16.2948,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.5664,120.603,'X[2] <= 0.193\\nentropy = 0.567\\nsamples = 15\\nvalue = [13, 2]'),\n",
       " Text(16.4985,114.72,'X[8] <= 0.778\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(16.4306,108.837,'X[12] <= 0.5\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(16.3627,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(16.4985,102.954,'X[4] <= 0.377\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(16.4306,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.5664,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(16.5664,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.6343,114.72,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(16.4985,126.486,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(16.5664,132.369,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(15.531,167.668,'X[10] <= 0.5\\nentropy = 0.918\\nsamples = 6\\nvalue = [2, 4]'),\n",
       " Text(15.4631,161.785,'X[20] <= 0.364\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(15.3952,155.902,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(15.531,155.902,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(15.5989,161.785,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(16.7977,173.551,'X[11] <= 0.059\\nentropy = 0.844\\nsamples = 103\\nvalue = [75, 28]'),\n",
       " Text(16.4985,167.668,'X[7] <= 0.016\\nentropy = 0.297\\nsamples = 19\\nvalue = [18, 1]'),\n",
       " Text(16.4306,161.785,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.5664,161.785,'entropy = 0.0\\nsamples = 18\\nvalue = [18, 0]'),\n",
       " Text(17.0969,167.668,'X[7] <= 0.109\\nentropy = 0.906\\nsamples = 84\\nvalue = [57, 27]'),\n",
       " Text(16.7022,161.785,'X[10] <= 0.7\\nentropy = 0.637\\nsamples = 31\\nvalue = [26, 5]'),\n",
       " Text(16.6343,155.902,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(16.7701,155.902,'X[20] <= 0.455\\nentropy = 0.874\\nsamples = 17\\nvalue = [12, 5]'),\n",
       " Text(16.6343,150.018,'X[17] <= 0.725\\nentropy = 0.619\\nsamples = 13\\nvalue = [11, 2]'),\n",
       " Text(16.5664,144.135,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(16.7022,144.135,'X[4] <= 0.366\\nentropy = 0.811\\nsamples = 8\\nvalue = [6, 2]'),\n",
       " Text(16.6343,138.252,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(16.7701,138.252,'X[9] <= 0.667\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(16.7022,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.838,132.369,'X[3] <= 0.165\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(16.7701,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(16.9059,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(16.9059,150.018,'X[11] <= 0.205\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(16.838,144.135,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(16.9738,144.135,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(17.4915,161.785,'X[2] <= 0.24\\nentropy = 0.979\\nsamples = 53\\nvalue = [31, 22]'),\n",
       " Text(17.1096,155.902,'X[12] <= 0.75\\nentropy = 0.881\\nsamples = 10\\nvalue = [3, 7]'),\n",
       " Text(17.0417,150.018,'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(17.1775,150.018,'X[1] <= 0.008\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(17.1096,144.135,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.2454,144.135,'X[13] <= 0.333\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(17.1775,138.252,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(17.3133,138.252,'X[0] <= 0.25\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(17.2454,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.3812,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(17.8734,155.902,'X[11] <= 0.284\\nentropy = 0.933\\nsamples = 43\\nvalue = [28, 15]'),\n",
       " Text(17.4491,150.018,'X[4] <= 0.348\\nentropy = 0.98\\nsamples = 12\\nvalue = [5, 7]'),\n",
       " Text(17.3812,144.135,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.517,144.135,'X[4] <= 0.355\\nentropy = 0.994\\nsamples = 11\\nvalue = [5, 6]'),\n",
       " Text(17.4491,138.252,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(17.5848,138.252,'X[7] <= 0.147\\nentropy = 0.971\\nsamples = 10\\nvalue = [4, 6]'),\n",
       " Text(17.517,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.6527,132.369,'X[1] <= 0.018\\nentropy = 0.991\\nsamples = 9\\nvalue = [4, 5]'),\n",
       " Text(17.5848,126.486,'X[1] <= 0.003\\nentropy = 1.0\\nsamples = 8\\nvalue = [4, 4]'),\n",
       " Text(17.4151,120.603,'X[9] <= 0.833\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(17.3472,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(17.483,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.7546,120.603,'X[12] <= 0.75\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(17.6188,114.72,'X[1] <= 0.009\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(17.5509,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(17.6867,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(17.8904,114.72,'X[20] <= 0.273\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(17.8225,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(17.9583,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(17.7206,126.486,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.2977,150.018,'X[13] <= 0.5\\nentropy = 0.824\\nsamples = 31\\nvalue = [23, 8]'),\n",
       " Text(18.162,144.135,'X[20] <= 0.5\\nentropy = 0.592\\nsamples = 21\\nvalue = [18, 3]'),\n",
       " Text(18.0941,138.252,'entropy = 0.0\\nsamples = 11\\nvalue = [11, 0]'),\n",
       " Text(18.2299,138.252,'X[11] <= 0.846\\nentropy = 0.881\\nsamples = 10\\nvalue = [7, 3]'),\n",
       " Text(18.162,132.369,'X[2] <= 0.265\\nentropy = 0.764\\nsamples = 9\\nvalue = [7, 2]'),\n",
       " Text(18.0941,126.486,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(18.2299,126.486,'X[8] <= 0.389\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(18.162,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(18.2977,120.603,'X[2] <= 0.276\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(18.162,114.72,'X[18] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(18.0941,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(18.2299,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.4335,114.72,'X[12] <= 0.75\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(18.3656,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(18.5014,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.2977,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.4335,144.135,'X[7] <= 0.113\\nentropy = 1.0\\nsamples = 10\\nvalue = [5, 5]'),\n",
       " Text(18.3656,138.252,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(18.5014,138.252,'X[7] <= 0.196\\nentropy = 0.954\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(18.4335,132.369,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(18.5693,132.369,'X[8] <= 0.389\\nentropy = 0.971\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(18.5014,126.486,'X[20] <= 0.136\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(18.4335,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.5693,120.603,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(18.6372,126.486,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(37.3668,179.434,'X[2] <= 0.229\\nentropy = 0.812\\nsamples = 1267\\nvalue = [950, 317]'),\n",
       " Text(30.1434,173.551,'X[7] <= 0.323\\nentropy = 0.787\\nsamples = 1194\\nvalue = [913, 281]'),\n",
       " Text(23.1,167.668,'X[3] <= 0.039\\nentropy = 0.706\\nsamples = 698\\nvalue = [564, 134]'),\n",
       " Text(18.5014,161.785,'X[12] <= 0.25\\nentropy = 0.997\\nsamples = 15\\nvalue = [8, 7]'),\n",
       " Text(18.4335,155.902,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(18.5693,155.902,'X[1] <= 0.013\\nentropy = 0.946\\nsamples = 11\\nvalue = [4, 7]'),\n",
       " Text(18.5014,150.018,'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(18.6372,150.018,'X[13] <= 0.333\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(18.5693,144.135,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(18.7051,144.135,'X[4] <= 0.428\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(18.6372,138.252,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.773,138.252,'X[8] <= 0.389\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(18.7051,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(18.8409,132.369,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(27.6986,161.785,'X[13] <= 0.167\\nentropy = 0.693\\nsamples = 683\\nvalue = [556, 127]'),\n",
       " Text(24.8507,155.902,'X[1] <= 0.013\\nentropy = 0.738\\nsamples = 476\\nvalue = [377, 99]'),\n",
       " Text(22.2293,150.018,'X[20] <= 0.5\\nentropy = 0.776\\nsamples = 354\\nvalue = [273, 81]'),\n",
       " Text(20.0333,144.135,'X[14] <= 0.222\\nentropy = 0.692\\nsamples = 205\\nvalue = [167, 38]'),\n",
       " Text(19.0446,138.252,'X[3] <= 0.124\\nentropy = 0.918\\nsamples = 30\\nvalue = [20, 10]'),\n",
       " Text(18.9767,132.369,'X[7] <= 0.234\\nentropy = 0.779\\nsamples = 26\\nvalue = [20, 6]'),\n",
       " Text(18.9088,126.486,'X[7] <= 0.228\\nentropy = 0.9\\nsamples = 19\\nvalue = [13, 6]'),\n",
       " Text(18.8409,120.603,'X[4] <= 0.458\\nentropy = 0.787\\nsamples = 17\\nvalue = [13, 4]'),\n",
       " Text(18.773,114.72,'X[7] <= 0.035\\nentropy = 0.946\\nsamples = 11\\nvalue = [7, 4]'),\n",
       " Text(18.7051,108.837,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(18.8409,108.837,'X[4] <= 0.451\\nentropy = 0.764\\nsamples = 9\\nvalue = [7, 2]'),\n",
       " Text(18.773,102.954,'X[4] <= 0.415\\nentropy = 0.544\\nsamples = 8\\nvalue = [7, 1]'),\n",
       " Text(18.7051,97.0708,'X[1] <= 0.007\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(18.6372,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(18.773,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.8409,97.0708,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(18.9088,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(18.9088,114.72,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(18.9767,120.603,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(19.0446,126.486,'entropy = 0.0\\nsamples = 7\\nvalue = [7, 0]'),\n",
       " Text(19.1125,132.369,'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(21.022,138.252,'X[1] <= 0.011\\nentropy = 0.634\\nsamples = 175\\nvalue = [147, 28]'),\n",
       " Text(20.1139,132.369,'X[2] <= 0.206\\nentropy = 0.551\\nsamples = 133\\nvalue = [116, 17]'),\n",
       " Text(20.046,126.486,'X[9] <= 0.167\\nentropy = 0.589\\nsamples = 120\\nvalue = [103, 17]'),\n",
       " Text(19.3501,120.603,'X[7] <= 0.072\\nentropy = 0.759\\nsamples = 41\\nvalue = [32, 9]'),\n",
       " Text(19.0446,114.72,'X[4] <= 0.474\\nentropy = 0.353\\nsamples = 15\\nvalue = [14, 1]'),\n",
       " Text(18.9767,108.837,'entropy = 0.0\\nsamples = 13\\nvalue = [13, 0]'),\n",
       " Text(19.1125,108.837,'X[14] <= 0.905\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(19.0446,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(19.1804,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(19.6556,114.72,'X[1] <= 0.005\\nentropy = 0.89\\nsamples = 26\\nvalue = [18, 8]'),\n",
       " Text(19.3841,108.837,'X[7] <= 0.271\\nentropy = 0.996\\nsamples = 13\\nvalue = [7, 6]'),\n",
       " Text(19.3162,102.954,'X[7] <= 0.098\\nentropy = 0.994\\nsamples = 11\\nvalue = [5, 6]'),\n",
       " Text(19.2483,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(19.3841,97.0708,'X[18] <= 0.286\\nentropy = 1.0\\nsamples = 10\\nvalue = [5, 5]'),\n",
       " Text(19.2483,91.1877,'X[3] <= 0.101\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(19.1804,85.3046,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(19.3162,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(19.5199,91.1877,'X[3] <= 0.098\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(19.452,85.3046,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(19.5878,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(19.452,102.954,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(19.9272,108.837,'X[20] <= 0.136\\nentropy = 0.619\\nsamples = 13\\nvalue = [11, 2]'),\n",
       " Text(19.8593,102.954,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(19.9951,102.954,'X[20] <= 0.318\\nentropy = 0.811\\nsamples = 8\\nvalue = [6, 2]'),\n",
       " Text(19.9272,97.0708,'X[3] <= 0.08\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(19.7914,91.1877,'X[10] <= 0.4\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(19.7235,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(19.8593,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.063,91.1877,'X[18] <= 0.286\\nentropy = 0.811\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(19.9951,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(20.1309,85.3046,'X[7] <= 0.244\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(20.063,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(20.1988,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.063,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(20.742,120.603,'X[3] <= 0.116\\nentropy = 0.473\\nsamples = 79\\nvalue = [71, 8]'),\n",
       " Text(20.4704,114.72,'X[11] <= 0.205\\nentropy = 0.371\\nsamples = 70\\nvalue = [65, 5]'),\n",
       " Text(20.4025,108.837,'entropy = 0.0\\nsamples = 17\\nvalue = [17, 0]'),\n",
       " Text(20.5383,108.837,'X[20] <= 0.136\\nentropy = 0.451\\nsamples = 53\\nvalue = [48, 5]'),\n",
       " Text(20.4025,102.954,'X[1] <= 0.002\\nentropy = 0.619\\nsamples = 26\\nvalue = [22, 4]'),\n",
       " Text(20.3346,97.0708,'entropy = 0.0\\nsamples = 7\\nvalue = [7, 0]'),\n",
       " Text(20.4704,97.0708,'X[7] <= 0.033\\nentropy = 0.742\\nsamples = 19\\nvalue = [15, 4]'),\n",
       " Text(20.4025,91.1877,'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(20.5383,91.1877,'X[1] <= 0.007\\nentropy = 0.863\\nsamples = 14\\nvalue = [10, 4]'),\n",
       " Text(20.4704,85.3046,'X[19] <= 0.915\\nentropy = 0.89\\nsamples = 13\\nvalue = [9, 4]'),\n",
       " Text(20.3346,79.4215,'X[2] <= 0.067\\nentropy = 0.954\\nsamples = 8\\nvalue = [5, 3]'),\n",
       " Text(20.2667,73.5385,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(20.4025,73.5385,'X[4] <= 0.434\\nentropy = 0.971\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(20.3346,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.4704,67.6554,'X[18] <= 0.5\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(20.3346,61.7723,'X[20] <= 0.045\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(20.2667,55.8892,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(20.4025,55.8892,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.6062,61.7723,'X[7] <= 0.045\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(20.5383,55.8892,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.6741,55.8892,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(20.6062,79.4215,'X[7] <= 0.247\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(20.5383,73.5385,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(20.6741,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.6062,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(20.6741,102.954,'X[1] <= 0.008\\nentropy = 0.229\\nsamples = 27\\nvalue = [26, 1]'),\n",
       " Text(20.6062,97.0708,'entropy = 0.0\\nsamples = 22\\nvalue = [22, 0]'),\n",
       " Text(20.742,97.0708,'X[18] <= 0.286\\nentropy = 0.722\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(20.6741,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(20.8099,91.1877,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(21.0136,114.72,'X[10] <= 0.9\\nentropy = 0.918\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(20.9457,108.837,'X[12] <= 0.25\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(20.8778,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.0136,102.954,'X[1] <= 0.003\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(20.9457,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.0814,97.0708,'X[14] <= 0.667\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(20.9457,91.1877,'X[20] <= 0.136\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(20.8778,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.0136,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.2172,91.1877,'X[7] <= 0.034\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(21.1493,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.2851,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.0814,108.837,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(20.1818,126.486,'entropy = 0.0\\nsamples = 13\\nvalue = [13, 0]'),\n",
       " Text(21.9301,132.369,'X[16] <= 0.952\\nentropy = 0.83\\nsamples = 42\\nvalue = [31, 11]'),\n",
       " Text(21.8622,126.486,'X[9] <= 0.167\\nentropy = 0.888\\nsamples = 36\\nvalue = [25, 11]'),\n",
       " Text(21.5567,120.603,'X[4] <= 0.431\\nentropy = 0.592\\nsamples = 14\\nvalue = [12, 2]'),\n",
       " Text(21.4888,114.72,'X[3] <= 0.075\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(21.4209,108.837,'X[3] <= 0.067\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(21.2851,102.954,'X[19] <= 0.5\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(21.2172,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.353,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.5567,102.954,'X[11] <= 0.017\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(21.4888,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.6246,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.5567,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(21.6246,114.72,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(22.1678,120.603,'X[3] <= 0.124\\nentropy = 0.976\\nsamples = 22\\nvalue = [13, 9]'),\n",
       " Text(22.0999,114.72,'X[7] <= 0.298\\nentropy = 0.934\\nsamples = 20\\nvalue = [13, 7]'),\n",
       " Text(21.9641,108.837,'X[7] <= 0.286\\nentropy = 0.874\\nsamples = 17\\nvalue = [12, 5]'),\n",
       " Text(21.8962,102.954,'X[8] <= 0.722\\nentropy = 0.94\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(21.7604,97.0708,'X[20] <= 0.045\\nentropy = 0.764\\nsamples = 9\\nvalue = [7, 2]'),\n",
       " Text(21.6925,91.1877,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(21.8283,91.1877,'X[2] <= 0.101\\nentropy = 0.863\\nsamples = 7\\nvalue = [5, 2]'),\n",
       " Text(21.7604,85.3046,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(21.8962,85.3046,'X[7] <= 0.168\\nentropy = 0.971\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(21.7604,79.4215,'X[15] <= 0.224\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(21.6925,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(21.8283,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.032,79.4215,'X[12] <= 0.25\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(21.9641,73.5385,'X[7] <= 0.262\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(21.8962,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.032,67.6554,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(22.0999,73.5385,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.032,97.0708,'X[7] <= 0.134\\nentropy = 0.971\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(21.9641,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.0999,91.1877,'X[7] <= 0.262\\nentropy = 0.811\\nsamples = 4\\nvalue = [1, 3]'),\n",
       " Text(22.032,85.3046,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(22.1678,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.032,102.954,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(22.2357,108.837,'X[9] <= 0.667\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(22.1678,102.954,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(22.3036,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.2357,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(21.998,126.486,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(24.4253,144.135,'X[20] <= 0.591\\nentropy = 0.867\\nsamples = 149\\nvalue = [106, 43]'),\n",
       " Text(23.4408,138.252,'X[3] <= 0.121\\nentropy = 0.999\\nsamples = 31\\nvalue = [16, 15]'),\n",
       " Text(23.3729,132.369,'X[15] <= 0.79\\nentropy = 0.992\\nsamples = 29\\nvalue = [16, 13]'),\n",
       " Text(23.0844,126.486,'X[19] <= 0.915\\nentropy = 0.994\\nsamples = 22\\nvalue = [10, 12]'),\n",
       " Text(22.7109,120.603,'X[3] <= 0.054\\nentropy = 0.996\\nsamples = 13\\nvalue = [7, 6]'),\n",
       " Text(22.643,114.72,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.7788,114.72,'X[2] <= 0.139\\nentropy = 1.0\\nsamples = 12\\nvalue = [6, 6]'),\n",
       " Text(22.643,108.837,'X[16] <= 0.862\\nentropy = 0.918\\nsamples = 6\\nvalue = [2, 4]'),\n",
       " Text(22.5751,102.954,'X[2] <= 0.131\\nentropy = 0.971\\nsamples = 5\\nvalue = [2, 3]'),\n",
       " Text(22.5072,97.0708,'X[8] <= 0.389\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(22.3715,91.1877,'X[4] <= 0.433\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(22.3036,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(22.4394,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.643,91.1877,'X[3] <= 0.072\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(22.5751,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.7109,85.3046,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(22.643,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(22.7109,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(22.9146,108.837,'X[14] <= 0.234\\nentropy = 0.918\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(22.8467,102.954,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(22.9825,102.954,'X[10] <= 0.3\\nentropy = 1.0\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(22.8467,97.0708,'X[2] <= 0.16\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(22.7788,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(22.9146,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(23.1183,97.0708,'X[3] <= 0.103\\nentropy = 1.0\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(23.0504,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(23.1862,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(23.4578,120.603,'X[7] <= 0.091\\nentropy = 0.918\\nsamples = 9\\nvalue = [3, 6]'),\n",
       " Text(23.3899,114.72,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(23.5257,114.72,'X[1] <= 0.012\\nentropy = 0.985\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(23.4578,108.837,'X[11] <= 0.517\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(23.322,102.954,'X[1] <= 0.005\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(23.2541,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(23.3899,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(23.5936,102.954,'X[7] <= 0.175\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(23.5257,97.0708,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(23.6615,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(23.5936,108.837,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(23.6615,126.486,'X[3] <= 0.103\\nentropy = 0.592\\nsamples = 7\\nvalue = [6, 1]'),\n",
       " Text(23.5936,120.603,'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(23.7294,120.603,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(23.5087,132.369,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(25.4098,138.252,'X[4] <= 0.475\\nentropy = 0.791\\nsamples = 118\\nvalue = [90, 28]'),\n",
       " Text(25.1212,132.369,'X[4] <= 0.469\\nentropy = 0.819\\nsamples = 106\\nvalue = [79, 27]'),\n",
       " Text(24.7478,126.486,'X[1] <= 0.005\\nentropy = 0.791\\nsamples = 101\\nvalue = [77, 24]'),\n",
       " Text(24.2046,120.603,'X[14] <= 0.456\\nentropy = 0.65\\nsamples = 48\\nvalue = [40, 8]'),\n",
       " Text(24.1367,114.72,'entropy = 0.0\\nsamples = 12\\nvalue = [12, 0]'),\n",
       " Text(24.2725,114.72,'X[4] <= 0.451\\nentropy = 0.764\\nsamples = 36\\nvalue = [28, 8]'),\n",
       " Text(23.933,108.837,'X[16] <= 0.322\\nentropy = 0.529\\nsamples = 25\\nvalue = [22, 3]'),\n",
       " Text(23.8651,102.954,'X[2] <= 0.142\\nentropy = 0.845\\nsamples = 11\\nvalue = [8, 3]'),\n",
       " Text(23.7973,97.0708,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(23.933,97.0708,'X[1] <= 0.001\\nentropy = 0.985\\nsamples = 7\\nvalue = [4, 3]'),\n",
       " Text(23.8651,91.1877,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(24.0009,91.1877,'X[12] <= 0.25\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(23.8651,85.3046,'X[1] <= 0.004\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(23.7973,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(23.933,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(24.1367,85.3046,'X[1] <= 0.004\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(24.0688,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(24.2046,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(24.0009,102.954,'entropy = 0.0\\nsamples = 14\\nvalue = [14, 0]'),\n",
       " Text(24.612,108.837,'X[17] <= 0.225\\nentropy = 0.994\\nsamples = 11\\nvalue = [6, 5]'),\n",
       " Text(24.5441,102.954,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(24.6799,102.954,'X[15] <= 0.866\\nentropy = 0.971\\nsamples = 10\\nvalue = [6, 4]'),\n",
       " Text(24.612,97.0708,'X[3] <= 0.09\\nentropy = 0.918\\nsamples = 9\\nvalue = [6, 3]'),\n",
       " Text(24.5441,91.1877,'X[7] <= 0.198\\nentropy = 1.0\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(24.4083,85.3046,'X[8] <= 0.444\\nentropy = 0.918\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(24.3404,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(24.4762,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(24.6799,85.3046,'X[11] <= 0.517\\nentropy = 0.918\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(24.612,79.4215,'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(24.7478,79.4215,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(24.6799,91.1877,'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(24.7478,97.0708,'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(25.2909,120.603,'X[7] <= 0.087\\nentropy = 0.884\\nsamples = 53\\nvalue = [37, 16]'),\n",
       " Text(25.2231,114.72,'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(25.3588,114.72,'X[16] <= 0.114\\nentropy = 0.827\\nsamples = 50\\nvalue = [37, 13]'),\n",
       " Text(25.2909,108.837,'entropy = 0.0\\nsamples = 9\\nvalue = [9, 0]'),\n",
       " Text(25.4267,108.837,'X[7] <= 0.108\\nentropy = 0.901\\nsamples = 41\\nvalue = [28, 13]'),\n",
       " Text(25.3588,102.954,'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(25.4946,102.954,'X[7] <= 0.301\\nentropy = 0.935\\nsamples = 37\\nvalue = [24, 13]'),\n",
       " Text(25.3588,97.0708,'X[10] <= 0.9\\nentropy = 0.896\\nsamples = 32\\nvalue = [22, 10]'),\n",
       " Text(25.2909,91.1877,'X[7] <= 0.285\\nentropy = 0.94\\nsamples = 28\\nvalue = [18, 10]'),\n",
       " ...]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(Dec_clfb.fit(X_train_scb,Y_trainb)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- feature_4 <= 0.29\n",
      "|   |--- class: 0\n",
      "|--- feature_4 >  0.29\n",
      "|   |--- feature_1 <= 0.04\n",
      "|   |   |--- feature_3 <= 0.17\n",
      "|   |   |   |--- feature_4 <= 0.48\n",
      "|   |   |   |   |--- feature_0 <= 0.75\n",
      "|   |   |   |   |   |--- feature_4 <= 0.34\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |--- feature_2 <= 0.26\n",
      "|   |   |   |   |   |   |   |   |--- feature_10 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.33\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.33\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |--- feature_10 >  0.10\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.90\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_10 >  0.90\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_2 >  0.26\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.30\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 >  0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.64\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.64\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.30\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |--- feature_20 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |--- feature_12 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_12 >  0.25\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_20 >  0.14\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.34\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.46\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 >  0.46\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.34\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_4 >  0.34\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.40\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.12\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.99\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.70\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  0.70\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 15\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.99\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.12\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.06\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 >  0.06\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.11\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.11\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.40\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.23\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 22\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 18\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.23\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 11\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.11\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 15\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.11\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.28\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.28\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_0 >  0.75\n",
      "|   |   |   |   |   |--- feature_2 <= 0.19\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.39\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.12\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.23\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 >  0.23\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_19 <= 0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_19 >  0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.12\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.56\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.56\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.39\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.55\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.52\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 11\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.52\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.55\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_16 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.09\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.09\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_16 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 >  0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.06\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.06\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.44\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= 0.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  0.57\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.44\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- feature_2 >  0.19\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_4 >  0.48\n",
      "|   |   |   |   |--- feature_0 <= 0.75\n",
      "|   |   |   |   |   |--- feature_4 <= 0.63\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.56\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.07\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.77\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.26\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 11\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.26\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 11\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.77\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.07\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 24\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 15\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.56\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.36\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.36\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_2 >  0.18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |--- feature_2 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_11 <= 0.28\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_11 >  0.28\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.55\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.55\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.74\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.74\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_2 >  0.05\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 14\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_18 <= 0.29\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_18 >  0.29\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 14\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_4 >  0.63\n",
      "|   |   |   |   |   |   |--- feature_3 <= 0.04\n",
      "|   |   |   |   |   |   |   |--- feature_7 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 <= 0.23\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 21\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 14\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 >  0.23\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.79\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  0.79\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.77\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.77\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |--- feature_7 >  0.83\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.89\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.86\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_9 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_9 >  0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.89\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.98\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 >  0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_3 >  0.04\n",
      "|   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.84\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.33\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 26\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.33\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.15\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 28\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.15\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 14\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.84\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 11\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 >  0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 18\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.18\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.78\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 20\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.78\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.18\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.52\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.52\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_0 >  0.75\n",
      "|   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |--- feature_4 <= 0.69\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |--- feature_5 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 >  0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.82\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |--- feature_5 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.56\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_15 >  0.56\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  0.85\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.04\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 15\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.01\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_4 >  0.69\n",
      "|   |   |   |   |   |   |   |--- feature_7 <= 0.40\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_7 >  0.40\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.66\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.88\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.88\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.66\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_19 <= 0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_19 >  0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |--- feature_3 <= 0.11\n",
      "|   |   |   |   |   |   |   |--- feature_16 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_16 >  0.32\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.54\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.54\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 >  0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_3 >  0.11\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_3 >  0.17\n",
      "|   |   |   |--- feature_3 <= 0.39\n",
      "|   |   |   |   |--- feature_7 <= 0.33\n",
      "|   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |--- feature_3 <= 0.31\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.64\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.27\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 26\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 14\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.27\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.53\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.53\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.54\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.54\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.64\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.00\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.37\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.68\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  0.00\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.30\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.30\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_18 <= 0.79\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_18 >  0.79\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |--- feature_3 >  0.31\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.79\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 >  0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.79\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.14\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.36\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 >  0.36\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |--- feature_7 <= 0.19\n",
      "|   |   |   |   |   |   |   |--- feature_7 <= 0.18\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_7 >  0.18\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 <= 0.82\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_14 >  0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 <= 0.48\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_16 >  0.48\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |--- feature_20 >  0.82\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_7 >  0.19\n",
      "|   |   |   |   |   |   |   |--- feature_7 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |--- feature_15 <= 0.71\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.58\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.58\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 >  0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_15 >  0.71\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.70\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  0.70\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_7 >  0.32\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.30\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 >  0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.30\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- feature_7 >  0.33\n",
      "|   |   |   |   |   |--- feature_7 <= 0.67\n",
      "|   |   |   |   |   |   |--- feature_3 <= 0.30\n",
      "|   |   |   |   |   |   |   |--- feature_7 <= 0.56\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.55\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 16\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 >  0.93\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.55\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 <= 0.75\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.53\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  0.53\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 >  0.75\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_7 >  0.56\n",
      "|   |   |   |   |   |   |   |   |--- feature_14 <= 0.22\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.38\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.38\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_14 >  0.22\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_3 >  0.30\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_7 >  0.67\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.62\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.48\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.31\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.31\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 19\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 15\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.48\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.74\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.74\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_11 >  0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.62\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.61\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.94\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_8 >  0.61\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.32\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 8\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.84\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 12\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.84\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.89\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.89\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 10\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.18\n",
      "|   |   |   |   |   |   |   |   |--- feature_8 <= 0.72\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_8 >  0.72\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.03\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.18\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.45\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_3 >  0.39\n",
      "|   |   |   |   |--- feature_2 <= 0.56\n",
      "|   |   |   |   |   |--- feature_1 <= 0.02\n",
      "|   |   |   |   |   |   |--- feature_7 <= 0.21\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.89\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.54\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_4 >  0.67\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 13\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_3 >  0.47\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.54\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.89\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.19\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.19\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_17 <= 0.23\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_17 >  0.23\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_12 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_12 >  0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |--- feature_7 >  0.21\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.44\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.73\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_13 >  0.83\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.22\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.73\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 5\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.00\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.75\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_7 >  0.75\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.44\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_1 >  0.02\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_2 >  0.56\n",
      "|   |   |   |   |   |--- feature_7 <= 0.65\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_7 >  0.65\n",
      "|   |   |   |   |   |   |--- feature_7 <= 0.81\n",
      "|   |   |   |   |   |   |   |--- feature_1 <= 0.00\n",
      "|   |   |   |   |   |   |   |   |--- feature_16 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.71\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.35\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_15 >  0.35\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |--- feature_16 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 <= 0.64\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_14 >  0.64\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.66\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 6\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.66\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_1 >  0.00\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.80\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.80\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_12 >  0.25\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_7 >  0.81\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |--- feature_1 >  0.04\n",
      "|   |   |--- feature_1 <= 0.07\n",
      "|   |   |   |--- feature_3 <= 0.16\n",
      "|   |   |   |   |--- feature_4 <= 0.39\n",
      "|   |   |   |   |   |--- feature_7 <= 0.19\n",
      "|   |   |   |   |   |   |--- feature_11 <= 0.28\n",
      "|   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_11 >  0.28\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.06\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.06\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_15 <= 0.69\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_15 >  0.69\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.41\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_10 >  0.20\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.07\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |--- feature_7 >  0.19\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.05\n",
      "|   |   |   |   |   |   |   |--- feature_14 <= 0.93\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.35\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.35\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 <= 0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_8 >  0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.39\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_14 >  0.93\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.05\n",
      "|   |   |   |   |   |   |   |--- feature_1 <= 0.06\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_1 >  0.06\n",
      "|   |   |   |   |   |   |   |   |--- feature_8 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_8 >  0.50\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_4 >  0.39\n",
      "|   |   |   |   |   |--- feature_3 <= 0.03\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_3 >  0.03\n",
      "|   |   |   |   |   |   |--- feature_0 <= 0.75\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.68\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.66\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.14\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_20 >  0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.06\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_1 >  0.06\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.66\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 <= 0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_10 >  0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 7\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_13 >  0.17\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.68\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 <= 0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 9\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.59\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_11 >  0.63\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_1 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_0 >  0.75\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_3 >  0.16\n",
      "|   |   |   |   |--- feature_4 <= 0.41\n",
      "|   |   |   |   |   |--- feature_11 <= 0.28\n",
      "|   |   |   |   |   |   |--- feature_4 <= 0.36\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_4 >  0.36\n",
      "|   |   |   |   |   |   |   |--- feature_2 <= 0.39\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_2 >  0.39\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 <= 0.44\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 <= 0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_7 >  0.13\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 <= 0.27\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 4\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_20 >  0.27\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_2 >  0.44\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_11 >  0.28\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_4 >  0.41\n",
      "|   |   |   |   |   |--- feature_1 <= 0.04\n",
      "|   |   |   |   |   |   |--- feature_12 <= 0.75\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_12 >  0.75\n",
      "|   |   |   |   |   |   |   |--- feature_2 <= 0.28\n",
      "|   |   |   |   |   |   |   |   |--- feature_0 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_0 >  0.25\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_2 >  0.28\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_1 >  0.04\n",
      "|   |   |   |   |   |   |--- feature_9 <= 0.83\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_9 >  0.83\n",
      "|   |   |   |   |   |   |   |--- feature_4 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 <= 0.47\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_4 >  0.47\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_4 >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |--- feature_1 >  0.07\n",
      "|   |   |   |--- feature_4 <= 0.40\n",
      "|   |   |   |   |--- feature_1 <= 0.12\n",
      "|   |   |   |   |   |--- feature_12 <= 0.75\n",
      "|   |   |   |   |   |   |--- feature_1 <= 0.07\n",
      "|   |   |   |   |   |   |   |--- feature_8 <= 0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_8 >  0.50\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |--- feature_1 >  0.07\n",
      "|   |   |   |   |   |   |   |--- feature_12 <= 0.25\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 <= 0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_3 >  0.05\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 <= 0.35\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 <= 0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_2 >  0.16\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_4 >  0.35\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |--- feature_12 >  0.25\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 <= 0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_16 <= 0.48\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_16 >  0.48\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |--- feature_7 >  0.04\n",
      "|   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_12 >  0.75\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_1 >  0.12\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |   |--- feature_4 >  0.40\n",
      "|   |   |   |   |--- feature_1 <= 0.09\n",
      "|   |   |   |   |   |--- feature_20 <= 0.23\n",
      "|   |   |   |   |   |   |--- feature_4 <= 0.66\n",
      "|   |   |   |   |   |   |   |--- feature_3 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |--- feature_3 >  0.10\n",
      "|   |   |   |   |   |   |   |   |--- feature_11 <= 0.02\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.08\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |--- feature_11 >  0.02\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 <= 0.09\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 <= 0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |   |   |   |   |   |   |   |--- feature_3 >  0.10\n",
      "|   |   |   |   |   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |   |   |   |   |--- feature_1 >  0.09\n",
      "|   |   |   |   |   |   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |   |   |--- feature_4 >  0.66\n",
      "|   |   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |   |--- feature_20 >  0.23\n",
      "|   |   |   |   |   |   |--- class: 1\n",
      "|   |   |   |   |--- feature_1 >  0.09\n",
      "|   |   |   |   |   |--- class: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree.export import export_text\n",
    "\n",
    "r = export_text(Dec_clfb)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = list(range(5, 15, 2))\n",
    "min_samples_split = list(range(2, 10, 1))\n",
    "\n",
    "for k in min_samples_split:\n",
    "    train_scores, test_scores = validation_curve(DecisionTreeClassifier(min_samples_split=k), X_train_scb,Y_trainb, \n",
    "                                                 param_name=\"max_depth\",scoring=\"f1\", param_range=max_depth_range,  cv=10)                                         \n",
    "                                           \n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = np.mean(train_scores,axis=1)\n",
    "valid_scores= np.mean(test_scores,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17873a3ab38>]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_scores,color=\"b\")\n",
    "plt.plot(valid_scores,color=\"r\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "KNN_clfb = KNeighborsClassifier()\n",
    "KNN_clfb.fit(X_train_scb,Y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preKNNb=KNN_clfb.predict(X_test_scb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7106963388370423"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_testb, y_preKNNb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "log=LogisticRegression()\n",
    "param_name = 'C'\n",
    "param_range =  np.arange(1,100, 3)\n",
    "\n",
    "train_scores, valid_scores = validation_curve(log,X_train_scb,Y_trainb,param_name=param_name,\n",
    "                                             param_range=param_range,scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x120507df400>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(train_scores, axis=1),color='red')\n",
    "plt.plot(np.mean(valid_scores, axis=1),color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.NaÃ¯veBayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_clf = GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohammed\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:206: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_clf.fit(X_train_scb,Y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prenbb=gnb_clf.predict(X_test_scb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6837760229720029"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_testb, y_prenbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
